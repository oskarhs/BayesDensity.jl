<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Implementing new Bayesian density estimators · BayesDensity.jl</title><meta name="title" content="Implementing new Bayesian density estimators · BayesDensity.jl"/><meta property="og:title" content="Implementing new Bayesian density estimators · BayesDensity.jl"/><meta property="twitter:title" content="Implementing new Bayesian density estimators · BayesDensity.jl"/><meta name="description" content="Documentation for BayesDensity.jl."/><meta property="og:description" content="Documentation for BayesDensity.jl."/><meta property="twitter:description" content="Documentation for BayesDensity.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="BayesDensity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BayesDensity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../density_estimation_primer/">A primer on Bayesian nonparametric density estimation</a></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../api/general_api/">General API</a></li><li><a class="tocitem" href="../../api/plotting_api/">Plotting API</a></li></ul></li><li><span class="tocitem">Methods</span><ul><li><a class="tocitem" href="../../methods/">Index</a></li><li><a class="tocitem" href="../../methods/BSplineMixture/">BSplineMixture</a></li><li><a class="tocitem" href="../../methods/HistSmoother/">HistSmoother</a></li><li><a class="tocitem" href="../../methods/PitmanYorMixture/">PitmanYorMixture</a></li><li><a class="tocitem" href="../../methods/FiniteGaussianMixture/">FiniteGaussianMixture</a></li><li><a class="tocitem" href="../../methods/RandomFiniteGaussianMixture/">RandomFiniteGaussianMixture</a></li><li><a class="tocitem" href="../../methods/RandomBernsteinPoly/">RandomBernsteinPoly</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/naive_bayes/">Naive Bayes</a></li><li><a class="tocitem" href="../../examples/model_selection/">Model selection</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Implementing new Bayesian density estimators</a><ul class="internal"><li><a class="tocitem" href="#Bayesian-inference-for-Bernstein-densities."><span>Bayesian inference for Bernstein densities.</span></a></li><li><a class="tocitem" href="#Implementation"><span>Implementation</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Implementing new Bayesian density estimators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Implementing new Bayesian density estimators</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl/blob/main/docs/src/tutorials/add_new_models.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Implementing-new-Bayesian-density-estimators"><a class="docs-heading-anchor" href="#Implementing-new-Bayesian-density-estimators">Implementing new Bayesian density estimators</a><a id="Implementing-new-Bayesian-density-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-new-Bayesian-density-estimators" title="Permalink"></a></h1><p>The following page provides a tutorial on how to implement new Bayesian density estimators compatible with the BayesDensity.jl-package. Prior to reading this tutorial, one should have already familiarized oneself with the general package API, for instance by reading the <a href="../../api/general_api/#General-API">General API</a> documentation.</p><p>In order to be able to follow this tutorial, it is advantageous to have some prior exposure to data-augmentation schemes, Gibbs sampling and mean-field variational inference. A good introduction to all three topics can be found in <a href="../../references/#Bishop2006pattern">Bishop (2006)</a>.</p><div class="admonition is-info" id="Note-143f1b2a9a166cb4"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-143f1b2a9a166cb4" title="Permalink"></a></header><div class="admonition-body"><p>The focus of the following tutorial is to present how one can implement new Bayesian models in a <code>BayesDensity</code>-compatible way. As a result, the implementation presented here is by no means optimal in terms of computational efficiency or numerical stability for this particular example.</p></div></div><h2 id="Bayesian-inference-for-Bernstein-densities."><a class="docs-heading-anchor" href="#Bayesian-inference-for-Bernstein-densities.">Bayesian inference for Bernstein densities.</a><a id="Bayesian-inference-for-Bernstein-densities.-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-inference-for-Bernstein-densities." title="Permalink"></a></h2><p>For our tutorial, we will illustrate by focusing on a Bayesian Bernstein-type density estimator for data supported on the unit interval.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1" class="footnote-ref">[1]</a><span class="footnote-preview" id="fn-1"></span></sup> This section provides the theoretical background for the model we will later implement as an example, and can be skipped by readers who are more interested in the details of the implementation itself.</p><p>Given a positive integer <span>$K$</span>, we say that <span>$f$</span> is a Bernstein density if we can write</p><p class="math-container">\[f(x) = \sum_{k=1}^K \theta_k\, \varphi_k(x), \quad x\in [0, 1],\]</p><p>where <span>$\varphi_k(\cdot)$</span> is the density of the <span>$\mathrm{Beta}(k, K-k+1)$</span>-distribution and <span>$\boldsymbol{\theta} \in \{\boldsymbol{\vartheta}\in [0,1]^K \colon \sum_k \vartheta_k = 1 \}$</span>. The corresponding cumulative distribution function <span>$F$</span> is then</p><p class="math-container">\[F(x) = \sum_{k=1}^K \theta_k\, \int_{0}^x\varphi_k(t)\, \text{d}t, \quad x\in [0, 1],\]</p><p>owing to the linearity of the integral.</p><p>Our main motivation for considering the Bernstein model is that under mild regularity conditions on the true density <span>$f_0$</span>, the Bernstein density <span>$f$</span> can approximate <span>$f_0$</span> to an arbitrary degree of precision with respect to a suitable metric, such as the total variation distance, provided <span>$K$</span> is sufficiently large.</p><p>For a Bayesian treatment of the Bernstein density model, we impose a <span>$\mathrm{Dirichlet}(\boldsymbol{a})$</span>-prior distribution on <span>$\boldsymbol{\theta}$</span>, where <span>$\boldsymbol{a} = (a,a, \ldots, a)$</span> for some <span>$a&gt;0$</span>. Given an observed independent and identically distributed sample <span>$\boldsymbol{x} = (x_1, x_2, \ldots, x_n)$</span>, the likelihood of the observed sample under the Bernstein model for <span>$f$</span> is</p><p class="math-container">\[p(\boldsymbol{x}\,|\, \boldsymbol{\theta}) = \prod_{i=1}^n \sum_{k=1}^K \theta_k\, \varphi_k(x_i).\]</p><p>The form taken by the likelihood function above makes Bayesian inference challenging due to the fact that the resulting posterior distribution is analytically intractable. However, by augmenting the data with latent variables <span>$\boldsymbol{z} \in \{1,2,\ldots, K\}^n$</span>, it is possible to perform posterior inference very efficiently through Gibbs sampling or mean-field VI. To this end, we note that an equivalent formulation of the Bernstein density model is</p><p class="math-container">\[\begin{align*}
    x_i\,|\, \{z_i = k\} &amp;\sim \varphi_k(x_i), &amp;i = 1,\ldots, n,\\
    z_i\,|\, \boldsymbol{\theta} &amp;\sim \text{Multinomial}(1, \boldsymbol{\theta}), &amp;i = 1, \ldots, n,\\
    \boldsymbol{\theta} &amp;\sim \text{Dirichlet}_K(\boldsymbol{a}).
\end{align*}\]</p><p>When <span>$\boldsymbol{z}$</span> is marginalized out, we recover the Bernstein density likelihood, so the two model formulations are equivalent. However, as it turns out that Bayesian inference based on the augmented posterior <span>$p(\boldsymbol{\theta}, \boldsymbol{z}\,|\, \boldsymbol{x})$</span> is much simpler than working with the marginal posterior of <span>$\boldsymbol{\theta}$</span>, we work with the augmented formulation instead.</p><p>Under this data augmentation strategy it can then be shown that joint posterior of <span>$\boldsymbol{\theta}, \boldsymbol{z}$</span> is</p><p class="math-container">\[p(\boldsymbol{\theta}, \boldsymbol{z}\, |\, \boldsymbol{x}) \propto \prod_{k = 1}^K \theta_k^{N_k + a - 1} \prod_{i=1}^n \prod_{k=1}^K \varphi_k(x_i)^{\mathbf{1}_{\{k\}}(z_i)},\]</p><p>where <span>$\mathbf{1}_{\{k\}}(\cdot)$</span> is the indicator function and <span>$N_k = \sum_{i=1}^n \mathbf{1}_{\{k\}}(z_i)$</span>.</p><h3 id="Gibbs-sampling"><a class="docs-heading-anchor" href="#Gibbs-sampling">Gibbs sampling</a><a id="Gibbs-sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Gibbs-sampling" title="Permalink"></a></h3><p>To write down a Gibbs sampler for this model, we need to derive the full conditional distributions of <span>$\boldsymbol{\theta}$</span> and <span>$\boldsymbol{z}$</span>. In this case, direct inspection of the joint posterior shows that</p><p class="math-container">\[p(\boldsymbol{\theta}\, |\, \boldsymbol{z}, \boldsymbol{x}) = \mathrm{Dirichlet}(\boldsymbol{a} + \boldsymbol{N}),\]</p><p>where <span>$\boldsymbol{N} = (N_1, N_2, \ldots, N_K)$</span>. The full conditional distributions for <span>$\boldsymbol{z}$</span> are </p><p class="math-container">\[p(\boldsymbol{z}\, |\, \boldsymbol{\theta}, \boldsymbol{x}) \propto \prod_{i=1}^n \prod_{k=1}^K \big\{\theta_k\,\varphi_k(x_i)\big\}^{\mathbf{1}_{\{k\}}(z_i)}.\]</p><p>Hence, we see that <span>$z_1, \ldots, z_n$</span> are independent given <span>$\boldsymbol{\theta}, \boldsymbol{x}$</span>, with <span>$p(z_i = k \,|\, \boldsymbol{\theta}, \boldsymbol{x}) \propto \theta_k\, \varphi_k(x_i)$</span>.</p><h3 id="Variational-inference"><a class="docs-heading-anchor" href="#Variational-inference">Variational inference</a><a id="Variational-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Variational-inference" title="Permalink"></a></h3><p>For the Bernstein density , it is relatively straightforward to implement a mean-field variational inference scheme. Here, we approximate the joint posterior <span>$p(\boldsymbol{\theta}, \boldsymbol{z}\,|\, \boldsymbol{x})$</span> via a distribution <span>$q$</span> which satisfies the following independence assumption:</p><p class="math-container">\[q(\boldsymbol{\theta}, \boldsymbol{z}) = q(\boldsymbol{\theta})\, q(\boldsymbol{z}).\]</p><p>It can be shown [see e.g. <a href="../../references/#Ormerod2010explaining">Ormerod and Wand (2010)</a>] that the optimal <span>$q$</span> densities are given by</p><p class="math-container">\[\begin{aligned}
  q(\boldsymbol{\theta}) &amp;\propto \exp\big\{\mathbb{E}_{\boldsymbol{z}}\big[\log p(\boldsymbol{\theta}, \boldsymbol{z})\big]\big\},\\
  q(\boldsymbol{z}) &amp;\propto \exp\big\{\mathbb{E}_{\boldsymbol{\theta}}\big[\log p(\boldsymbol{\theta}, \boldsymbol{z})\big]\big\},
\end{aligned}\]</p><p>where the expectations are taken with respect to <span>$q(\boldsymbol{z})$</span> and <span>$q(\boldsymbol{\theta})$</span>, respectively. This result leads to the iterative coordinate-wise ascent variational inference algorithm (CAVI) for finding the optimal <span>$q$</span> densities, where we cyclically update <span>$q(\boldsymbol{\theta})$</span> and <span>$q(\boldsymbol{z})$</span> until some convergence criterion has been met. An oft-used convergence criterion for this purpose is the evidence lower bound, (ELBO):</p><p class="math-container">\[\mathrm{ELBO}(q) = \mathbb{E}_{\boldsymbol{\theta}, \boldsymbol{z}}\Big(\log \frac{p(\boldsymbol{x}, \boldsymbol{\theta}, \boldsymbol{z})}{q(\boldsymbol{\theta}, \boldsymbol{z})}\Big).\]</p><p>For our particular example it can be shown that the optimal <span>$q$</span> densities are:</p><ul><li><span>$q(\boldsymbol{\theta})$</span> is the <span>$\mathrm{Dirichlet}(\boldsymbol{a} + \boldsymbol{r})$</span>-density, where <span>$r_k = \sum_{i=1}^n q(z_i = k)$</span>.</li><li><span>$q(\boldsymbol{z}) = \prod_{i=1}^n q(z_i)$</span>, where <span>$q(z_i)$</span> is the probability mass function of a categorical distribution on <span>$\{1,2,\ldots, K\}$</span> with <span>$q(z_i = k) \propto \varphi_k(x_i)\, \exp\big\{\psi(a_k + r_k)\big\}$</span>, where <span>$\psi(\cdot)$</span> denotes the <a href="https://en.wikipedia.org/wiki/Digamma_function">digamma function</a>.</li></ul><p>An expression for the ELBO of this model is as follows:</p><p class="math-container">\[\begin{aligned}
  \mathrm{ELBO}(q) =&amp; \sum_{i=1}^n \sum_{k=1}^K q(z_i = k) \big\{\log b_k(x_i) - \log q(z_i = k)\big\} \\ &amp;+ \sum_{k=1}^K \big\{\log \Gamma(a + r_k) - \log \Gamma(a)\big\} \\ &amp;- \log \Gamma(aK+n) + \log \Gamma(aK)
\end{aligned}\]</p><h2 id="Implementation"><a class="docs-heading-anchor" href="#Implementation">Implementation</a><a id="Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation" title="Permalink"></a></h2><p>We start by importing the required packages:</p><pre><code class="language-julia hljs">using BayesDensityCore, Distributions, Random, StatsBase</code></pre><h3 id="Model-struct-and-pdf"><a class="docs-heading-anchor" href="#Model-struct-and-pdf">Model struct and pdf</a><a id="Model-struct-and-pdf-1"></a><a class="docs-heading-anchor-permalink" href="#Model-struct-and-pdf" title="Permalink"></a></h3><p>The first step to implementing the Bernstein density model in a <code>BayesDensity</code>-compatible way is to define a model struct which is a subtype of <a href="../../api/general_api/#BayesDensityCore.AbstractBayesDensityModel"><code>AbstractBayesDensityModel</code></a>:</p><pre><code class="language-julia hljs">struct BernsteinDensity{T&lt;:Real, D&lt;:NamedTuple} &lt;: AbstractBayesDensityModel{T}
    data::D # NamedTuple holding data
    K::Int  # Basis dimension
    a::T    # Symmetric Dirichlet parameter.
    function BernsteinDensity{T}(x::AbstractVector{&lt;:Real}, K::Int; a::Real=1.0) where {T&lt;:Real}
        φ_x = Matrix{T}(undef, (length(x), K))
        for i in eachindex(x)
            for k in 1:K
                φ_x[i, k] = pdf(Beta(k, K - k + 1), x[i])
            end
        end
        data = (x = x, n = length(x), φ_x = φ_x)
        return new{T, typeof(data)}(data, K, T(a))
    end
end
BernsteinDensity(args...; kwargs...) = BernsteinDensity{Float64}(args...; kwargs...) # For convenience</code></pre><p>In the above implementation, we store the values of <span>$\varphi_k(x_i)$</span> for <span>$1 \leq i \leq n$</span> and <span>$1 \leq k \leq K$</span>, as these values are reused repeatedly in the model fitting processes later. We also mantain a copy of the original dataset <code>x</code> in the <code>data</code> field. By default, the original data stored under the field <code>bdm.data.x</code> is used to select a default grid for the first coordinate axis when plotting fitted model objects via the fallback implementation of <a href="../../api/general_api/#BayesDensityCore.default_grid_points"><code>default_grid_points</code></a>. If the original data cannot be found under <code>bdm.data.x</code>, then trying to plot a <a href="../../api/general_api/#BayesDensityCore.PosteriorSamples"><code>PosteriorSamples</code></a> or <a href="../../api/general_api/#BayesDensityCore.AbstractVIPosterior"><code>AbstractVIPosterior</code></a> object without supplying a plotting grid as the second argument will fail, in which case implementing <code>default_plot_grid</code> for this model class will resolve this issue.</p><p>Returning to our specific example, we note that the Bernstein model is always supported on <span>$[0, 1]$</span>, so it may make more sense to use a default grid spanning this entire interval. A possible implementation of <code>default_grid_points</code> for this purpose is as follows:</p><pre><code class="language-julia hljs">BayesDensityCore.default_grid_points(::BernsteinDensity{T}) where {T} = LinRange{T}(0, 1, 2001)</code></pre><p>In order to be able use all the functionality of <code>BayesDensityCore</code>, we also need to implement an equality method for our new type. In this case, this is just a matter of checking that all the fields of two such objects are equal:</p><pre><code class="language-julia hljs">Base.:(==)(bd1::BernsteinDensity, bd2::BernsteinDensity) = bd1.data == bd2.data &amp;&amp; bd1.K == bd2.K &amp;&amp; bd1.a == bd2.a</code></pre><p>Next, we implement a method that calculates the pdf of the model when the parameters of the model are given. The <a href="../../api/general_api/#Distributions.pdf-Tuple{AbstractBayesDensityModel, Any, Real}"><code>pdf</code></a> method should always receive the model object as the first argument, the parameters as the second argument and the point(s) at which the density should be evaluated as the third. In the implementation presented below, we take in a <code>NamedTuple</code> with a single field named <code>θ</code> which represents the mixture probabilities.</p><pre><code class="language-julia hljs">function Distributions.pdf(bdm::BernsteinDensity{T, D}, params::NamedTuple, t::S) where {T&lt;:Real, D, S&lt;:Real}
    K = bdm.K
    (; θ) = params
    f = zero(promote_type(T, S))
    for k in 1:K
        f += θ[k] * pdf(Beta(k, K - k + 1), t)
    end
    return f
end</code></pre><p>The <code>BayesDensityCore</code> module provides generic fallback methods for the cases where <code>params</code> is given as a Vector of NamedTuples and when <code>t</code> is a vector. However, as noted in the general API, it is recommended that most models provide specialized methods for vectors of parameters and vectors of evaluation points, as it is often possible to implement batch evaluation more efficiently, e.g. by leveraging BLAS calls instead of loops, when the parameters and the evaluation grid are provided in batches.</p><p>Next, we need to implement the cdf method. Owing to the nice structure of the cdf <span>$F$</span> in this example, this is no more complicated than implementing the pdf:</p><pre><code class="language-julia hljs">function Distributions.cdf(bdm::BernsteinDensity{T, D}, params::NamedTuple, t::S) where {T&lt;:Real, D, S&lt;:Real}
    K = bdm.K
    (; θ) = params
    f = zero(promote_type(T, S))
    for k in 1:K
        f += θ[k] * cdf(Beta(k, K - k + 1), t)
    end
    return f
end</code></pre><p>In general, it is good practice to also implement the <a href="../../api/general_api/#Distributions.support-Union{Tuple{AbstractBayesDensityModel{T}}, Tuple{T}} where T"><code>support</code></a> and <a href="../../api/general_api/#BayesDensityCore.hyperparams-Tuple{AbstractBayesDensityModel}"><code>hyperparams</code></a> methods for new models. Note that for the Bernstein density model, the support is always equal to the unit interval, and the only hyperparameter is the scalar value <code>a</code> (here, we treat <code>K</code> as fixed). Hence, the following provides appropriate implementations of the aforementioned methods:</p><pre><code class="language-julia hljs">BayesDensityCore.support(::BernsteinDensity{T, D}) where {T, D} = (T(0.0), T(1.0))
BayesDensityCore.hyperparams(bdm::BernsteinDensity) = (a = bdm.a,)</code></pre><h3 id="Gibbs-sampler"><a class="docs-heading-anchor" href="#Gibbs-sampler">Gibbs sampler</a><a id="Gibbs-sampler-1"></a><a class="docs-heading-anchor-permalink" href="#Gibbs-sampler" title="Permalink"></a></h3><p>We now turn our attention to implementing the Gibbs sampler itself. All <code>BayesDensity</code>-compatible Markov chain Monte Carlo samplers should overload the <a href="../../api/general_api/#StatsBase.sample-Tuple{AbstractBayesDensityModel, Int64}"><code>sample</code></a> method. This function should always take in a random seed as the first argument, the density model object as the second argument and the total number of samples (including burn-in) as the third argument. In addition, the number of burn-in samples must be provided as a keyword argument.</p><p>The <code>sample</code> method should always return an object of type <code>PosteriorSamples</code> in order to be compatible with the rest of the package. The samples generated during the MCMC routine should be stored in a subtype of <code>AbstractVector</code>, where the type of the elements are compatible with the function signature for the implemented <code>pdf</code> method. Since our implementation of the <code>pdf</code> method takes in a NamedTuple as the <code>parameters</code> argument, we store the generated samples in a vector of NamedTuples in the implementation shown below:</p><pre><code class="language-julia hljs">function StatsBase.sample(rng::AbstractRNG, bdm::BernsteinDensity{T, D}, n_samples::Int; n_burnin=min(div(length(x), 5), 1000), init_params::NamedTuple=(θ = fill(1/K, K),)) where {T, D}
    (; K, data, a) = bdm
    (; x, n, φ_x) = data

    a_vec = fill(a, K) # Dirichlet prior parameter

    θ = T.(init_params.θ) # Initialize θ as the uniform vector
    probs = Vector{T}(undef, K) # Vector used to store intermediate calculations of p(zᵢ|θ, x)

    # Store samples as a vector of NamedTuples
    samples = Vector{NamedTuple{(:θ,), Tuple{Vector{Float64}}}}(undef, n_samples)

    for m in 1:n_samples
        N = zeros(Int, K) # N[k] = number of z[i] equal to k.
        for i in 1:n
            for k in 1:K
                probs[k] = θ[k] * φ_x[i, k]
            end
            probs = probs / sum(probs)
            N .+= rand(rng, Multinomial(1, probs)) # sample zᵢ ∼ p(zᵢ|θ, x)
        end
        θ = rand(rng, Dirichlet(a_vec + N)) # sample θ ∼ p(θ|z, x)
        samples[m] = (θ = θ,) # store the current value of θ
    end
    return PosteriorSamples{T}(samples, bdm, n_samples, n_burnin)
end</code></pre><p>The above implementation allows the user to supply the initial value of <span>$\theta$</span> used when performing the first iteration of the Gibbs sampler, via a <code>NamedTuple</code> to match the data structure we use to store the samples.</p><div class="admonition is-info" id="Note-1004bed40462baaf"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-1004bed40462baaf" title="Permalink"></a></header><div class="admonition-body"><p>The convention adopted by the current set of <code>BayesDensity</code> models is that when during an MCMC run, only model pararameters should be stored, and not auxilliary variables which are only introduced in order to facilitate efficient computation. In this case, we therefore do not store the <span>$z_i$</span> in the model object returned by this method.</p></div></div><h4 id="Example-usage"><a class="docs-heading-anchor" href="#Example-usage">Example usage</a><a id="Example-usage-1"></a><a class="docs-heading-anchor-permalink" href="#Example-usage" title="Permalink"></a></h4><p>Having implemented the model struct and the <code>pdf</code>- and <code>sample</code> methods, we can run the MCMC algorithm and perform posterior inference as with any of the other density esitmators implemented in this package:</p><pre><code class="language-julia hljs">d_true = Kumaraswamy(2, 5) # Simulate some data from a density supported on [0, 1]
rng = Xoshiro(1) # for reproducibility
x = rand(rng, d_true, 1000)

K = 20
bdm = BernsteinDensity(x, K) # Create Bernstein density model object (a = 1)
ps = sample(rng, bdm, 1_000; n_burnin=500) # Run MCMC

median(ps, 0.5) # Compute the posterior median of f(0.5)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.6636159017061116</code></pre><p>For instance, we can visualize the posterior fit by plotting the posterior means of <span>$f(t)$</span> and <span>$F(t)$</span> along with 95 % pointwise credible bands:</p><pre><code class="language-julia hljs">using CairoMakie
t = LinRange(0, 1, 1001) # Grid for plotting

fig = Figure(size=(600, 320))
ax1 = Axis(fig[1,1], xlabel=&quot;x&quot;, ylabel=&quot;Density&quot;)
plot!(ax1, ps, t, label=&quot;MCMC&quot;) # Plot the posterior mean and credible bands:
lines!(ax1, t, pdf(d_true, t), label=&quot;Truth&quot;, color=:black) # Also plot truth for comparison

ax2 = Axis(fig[1,2], xlabel=&quot;x&quot;, ylabel=&quot;Cumulative distribution&quot;)
plot!(ax2, ps, cdf, t, label=&quot;MCMC&quot;)
lines!(ax2, t, cdf(d_true, t), label=&quot;Truth&quot;, color=:black)

Legend(fig[1,3], ax1, framevisible=false)

fig</code></pre><p><img src="../../assets/bernstein_tutorial/bernstein_mcmc.svg" alt="Bernstein MCMC fit"/></p><h3 id="Mean-field-variational-inference"><a class="docs-heading-anchor" href="#Mean-field-variational-inference">Mean-field variational inference</a><a id="Mean-field-variational-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Mean-field-variational-inference" title="Permalink"></a></h3><p>Before getting started on implementing a variational inference algorithm, we first need to define a new struct that represents the variational posterior distribution. To make the resulting variational posterior compatible with the <code>BayesDensityCore</code> interface, the new struct should be a subtype of <a href="../../api/general_api/#BayesDensityCore.AbstractVIPosterior"><code>AbstractVIPosterior</code></a>.</p><p>Although not strictly required in order to make the variational posterior struct <code>BayesDensity</code>-compatible, it is customary to have the variational posterior distribution store the variational densities as <code>Distributions</code>-objects, in addition to storing the original model object.</p><p>The implementation below stores the variational density <span>$q(\boldsymbol{\theta})$</span>, along with the Bernstein-density model object. We also create a default constructor which takes in the vector <span>$\boldsymbol{r}$</span> resulting from the CAVI algorithm, along with the original model object:</p><pre><code class="language-julia hljs">struct BernsteinDensityVIPosterior{T&lt;:Real, D&lt;:Dirichlet{T}, M&lt;:BernsteinDensity} &lt;: AbstractVIPosterior{T}
    q_θ::D
    model::M
    function BernsteinDensityVIPosterior{T}(r::AbstractVector{&lt;:Real}, model::M) where {T&lt;:Real, M&lt;:BernsteinDensity}
        a = hyperparams(model).a
        K = model.K
        q_θ = Dirichlet{T}(fill(a, K) + r)
        return new{T, Dirichlet{T}, M}(q_θ, model)
    end
end</code></pre><p>It is also recommended to implement the <a href="../../api/general_api/#BayesDensityCore.model-Tuple{PosteriorSamples}"><code>model</code></a> method, so that the user can easily extract the model to which the variational posterior was fitted:</p><pre><code class="language-julia hljs">BayesDensityCore.model(vip::BernsteinDensityVIPosterior) = vip.model</code></pre><p>Next, we need to implement a method for generating samples from the variational posterior distribution, i.e. sampling from <span>$q(\boldsymbol{\theta})$</span>. This is achieved by implementing the <a href="../../api/general_api/#StatsBase.sample-Tuple{AbstractBayesDensityModel, Int64}"><code>sample</code></a> method:</p><pre><code class="language-julia hljs">function StatsBase.sample(rng::AbstractRNG, vip::BernsteinDensityVIPosterior{T,D, M}, n_samples::Int) where {T, D, M}
    q_θ = vip.q_θ
    samples = Vector{NamedTuple{(:θ,), Tuple{Vector{Float64}}}}(undef, n_samples)
    for m in 1:n_samples
        θ = rand(rng, q_θ)
        samples[m] = (θ = θ,)
    end
    # Note that we return independent samples here, so burn-in is not needed
    return PosteriorSamples{T}(samples, model(vip), n_samples, 0)
end</code></pre><p>Having implemented a struct for storing the variational posterior, we can now turn our attention to the optimization procedure itself. To start, we implement the ELBO, which we will need to determine convergence later. Note that in the implementation below, we assume that the values of <span>$q(z_i = k)$</span> are stored in a <span>$n \times K$</span> matrix <span>$\omega$</span>, so that <span>$\omega_{i,k} = q(z_i = k)$</span>.</p><pre><code class="language-julia hljs">function Bernstein_ELBO(model::BernsteinDensity{T, D}, r::AbstractVector{&lt;:Real}, ω::AbstractMatrix{&lt;:Real}) where {T, D}
    (; data, K, a) = model
    (; x, n, φ_x) = data
    logφ_x = log.(φ_x)
    ELBO = loggamma(a*K) - loggamma(a*K+n)
    ELBO += sum(loggamma.(r .+ a)) - K*loggamma(a)
    for k in 1:K
        for i in 1:n
            ELBO += ω[i,k]*(logφ_x[i,k] - log(ω[i,k]))
        end
    end
    return ELBO
end</code></pre><p>Finally, we are ready to implement the optimization procedure itself by overloading <a href="../../api/general_api/#BayesDensityCore.varinf-Tuple{AbstractBayesDensityModel}"><code>varinf</code></a>. Note that this function should return a 2-tuple, consisting of the fitted variational posterior itself and an object of type <a href="../../api/general_api/#BayesDensityCore.VariationalOptimizationResult"><code>VariationalOptimizationResult</code></a>.</p><pre><code class="language-julia hljs">using SpecialFunctions # For the digamma-function

function BayesDensityCore.varinf(model::BernsteinDensity{T, D}; max_iter::Int=1000, rtol::Real=1e-4) where {T, D}
    (; data, K, a) = model
    (; x, n, φ_x) = data

    # Initialize the latent variables ω[i,k] = q(z_i = k) to 1/K:
    ω = fill(T(1/K), (n, K))
    r = fill(a + n/K, K)

    # CAVI optimization loop
    ELBO_prev = T(-1)
    ELBO = Vector{T}(undef, max_iter)
    converged = false
    iter = 1
    while !converged &amp;&amp; iter &lt;= max_iter
        # Update q(θ)
        r = fill(a, K) + vec(sum(ω, dims=1))

        # Update q(z)
        for i in 1:n
            # Compute q(z_i = k) up to proportionality
            for k in 1:K
               ω[i,k] = φ_x[i,k] * exp(digamma(a + r[k]))
            end
            # Normalize so that the rows of ω sum to 1:
            ω[i,:] = ω[i,:] / sum(ω[i,:])
        end

        # Check if the procedure has converged:
        ELBO[iter] = Bernstein_ELBO(model, r, ω)

        # Run at least two iterations
        converged = (abs(ELBO_prev - ELBO[iter]) / ELBO_prev &lt;= rtol) &amp;&amp; iter &gt; 1
        ELBO_prev = ELBO[iter]
        iter += 1
    end

    # Print a warning if the procedure fails to converge within the maximum number of iterations
    converged || @warn &quot;Maximum number of iterations reached.&quot;

    posterior = BernsteinDensityVIPosterior{T}(r, model)
    info = VariationalOptimizationResult{T}(ELBO[1:iter-1], converged, iter-1, rtol, posterior)
    return posterior, info
end</code></pre><h4 id="Example-usage-2"><a class="docs-heading-anchor" href="#Example-usage-2">Example usage</a><a class="docs-heading-anchor-permalink" href="#Example-usage-2" title="Permalink"></a></h4><p>Having implemented the <code>varinf</code> method, we can now perform variational inference for the <code>BernsteinDensity</code> model just as easily as for any other <code>BayesDensityCore</code>-compatible model. The example below shows how to fit a variational posterior to the Bernstein model for a simulated dataset:</p><pre><code class="language-julia hljs">d_true = Kumaraswamy(2, 5) # Simulate some data from a density supported on [0, 1]
rng = Xoshiro(1) # for reproducibility
x = rand(rng, d_true, 1_000)

K = 20
bdm = BernsteinDensity(x, K) # Create Bernstein density model object (a = 1)
vip, info = varinf(bdm) # Compute the variational posterior.

mean(vip, 0.2) # Compute the posterior mean of f(0.2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.5823253092661311</code></pre><p>For instance, we can visualize the variational posterior fit by displaying the (variational) posterior means of <span>$f(t)$</span> and <span>$F(t)$</span> along with 95 % pointwise credible bands:</p><pre><code class="language-julia hljs">using CairoMakie
t = LinRange(0, 1, 1001) # Grid for plotting

fig = Figure(size=(600, 320))
ax1 = Axis(fig[1,1], xlabel=&quot;x&quot;, ylabel=&quot;Density&quot;)
plot!(ax1, vip, t, label=&quot;VI&quot;) # Plot the posterior mean and credible bands:
lines!(ax1, t, pdf(d_true, t), label=&quot;Truth&quot;, color=:black) # Also plot truth for comparison

ax2 = Axis(fig[1,2], xlabel=&quot;x&quot;, ylabel=&quot;Cumulative distribution&quot;)
plot!(ax2, vip, cdf, t, label=&quot;VI&quot;)
lines!(ax2, t, cdf(d_true, t), label=&quot;Truth&quot;, color=:black)

Legend(fig[1,3], ax1, framevisible=false)

fig</code></pre><p><img src="../../assets/bernstein_tutorial/bernstein_varinf.svg" alt="Bernstein variational fit"/></p><p>We can also verify that the ELBO has converged:</p><pre><code class="language-julia hljs">fig = Figure(size=(400, 350))
ax = Axis(fig[1,1], xlabel=&quot;Iteration&quot;, ylabel=&quot;ELBO&quot;)
lines!(ax, info)
fig</code></pre><p><img src="../../assets/bernstein_tutorial/bernstein_elbo.svg" alt="Bernstein elbo"/></p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>A Bayesian Bernstein-type estimator, where the number <span>$K$</span> of mixture components is treated as a further random variable has been proposed by <a href="../../references/#Petrone1999bernstein">Petrone (1999)</a>. A <code>BayesDensity</code> implementation of this model is available as <a href="../../methods/RandomBernsteinPoly/#RandomBernsteinPoly"><code>RandomBernsteinPoly</code></a>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../examples/model_selection/">« Model selection</a><a class="docs-footer-nextpage" href="../../contributing/">Contributing »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Sunday 15 February 2026 16:58">Sunday 15 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
