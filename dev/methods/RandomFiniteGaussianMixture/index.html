<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RandomFiniteGaussianMixture · BayesDensity.jl</title><meta name="title" content="RandomFiniteGaussianMixture · BayesDensity.jl"/><meta property="og:title" content="RandomFiniteGaussianMixture · BayesDensity.jl"/><meta property="twitter:title" content="RandomFiniteGaussianMixture · BayesDensity.jl"/><meta name="description" content="Documentation for BayesDensity.jl."/><meta property="og:description" content="Documentation for BayesDensity.jl."/><meta property="twitter:description" content="Documentation for BayesDensity.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="BayesDensity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BayesDensity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../density_estimation_primer/">A primer on Bayesian nonparametric density estimation</a></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../api/general_api/">General API</a></li><li><a class="tocitem" href="../../api/plotting_api/">Plotting API</a></li></ul></li><li><span class="tocitem">Methods</span><ul><li><a class="tocitem" href="../">Index</a></li><li><a class="tocitem" href="../BSplineMixture/">BSplineMixture</a></li><li><a class="tocitem" href="../HistSmoother/">HistSmoother</a></li><li><a class="tocitem" href="../PitmanYorMixture/">PitmanYorMixture</a></li><li><a class="tocitem" href="../FiniteGaussianMixture/">FiniteGaussianMixture</a></li><li class="is-active"><a class="tocitem" href>RandomFiniteGaussianMixture</a><ul class="internal"><li><a class="tocitem" href="#Module-API"><span>Module API</span></a></li></ul></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/add_new_models/">Implementing new Bayesian density estimators</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Methods</a></li><li class="is-active"><a href>RandomFiniteGaussianMixture</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RandomFiniteGaussianMixture</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl/blob/main/docs/src/methods/RandomFiniteGaussianMixture.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="RandomFiniteGaussianMixture"><a class="docs-heading-anchor" href="#RandomFiniteGaussianMixture">RandomFiniteGaussianMixture</a><a id="RandomFiniteGaussianMixture-1"></a><a class="docs-heading-anchor-permalink" href="#RandomFiniteGaussianMixture" title="Permalink"></a></h1><p>Documentation for finite Gaussian mixture models, with a variable (random) number of mixture components.</p><p>This model is available through the <code>BayesDensityFiniteGaussianMixture</code> package.</p><p>The variational inference algorithm used to compute the posterior first proceeds by separately fitting mixture models for different values of <span>$K$</span>, recording the corresponding value of the optimized evidence lower bound, <span>$\mathrm{ELBO}(K)$</span> at the end of each optimization. The posterior over the number of mixture components <span>$p(K\,|\, \boldsymbol{x})$</span> is then approximated via</p><p class="math-container">\[q(K) \propto p(K)\,\exp\big\{\mathrm{ELBO}(K)\big\}.\]</p><p>This approximation can be justified in light of the fact that the ELBO is a lower bound on the log-marginal likelihood <span>$p(\boldsymbol{x}, K)$</span>. The approximate posterior for the number of mixture components together with the optimal variational densities given <span>$K$</span> defines a distribution over a space of mixture of variable dimension, which is then used to make inferences about the density of the given sample.</p><p>The algorithm used to compute the conditional variational posterior <span>$q(\boldsymbol{\mu}|k)\,q(\boldsymbol{\sigma}^2|k)\,q(\boldsymbol{w}|k)$</span> is variant of the algorithm 5 in <a href="../../references/#Ormerod2010explaining">Ormerod and Wand (2010)</a>. Note that our version also includes an additional hyperprior on the rate parameters of the mixture scales.</p><p>There are two main ways of proceeding with Bayesian inference for the variational posterior. One possibility is to proceed with the single value <span>$\hat{K}$</span> that maximizes the variational probability <span>$q(K)$</span>, the so-called maximum a posteriori model. Posterior inference then proceeds via the conditional variational posterior <span>$q\big(\boldsymbol{\mu}, \boldsymbol{\sigma}^2, \boldsymbol{w} | \hat{K}\big)$</span>. This model can be retrieved by utilizing the <a href="#BayesDensityFiniteGaussianMixture.maximum_a_posteriori"><code>maximum_a_posteriori</code></a> method on a fitted variational posterior, which can then be used for posterior inference.</p><p>Another possibility is to take a fully Bayesian approach, where we do not condition on a single value of <span>$K$</span>, but treat it as a random variable. To pursure this approach to posterior inference, one can simply use the object returned by calling <a href="../../api/general_api/#BayesDensityCore.varinf-Tuple{AbstractBayesDensityModel}"><code>varinf</code></a> directly (e.g. for plotting or computing other posterior summary statistics).</p><h2 id="Module-API"><a class="docs-heading-anchor" href="#Module-API">Module API</a><a id="Module-API-1"></a><a class="docs-heading-anchor-permalink" href="#Module-API" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture"><code>BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RandomFiniteGaussianMixture{T&lt;:Real} &lt;: AbstractBayesDensityModel{T}</code></pre><p>Struct representing a finite Gaussian mixture model with a variable (random) number of components.</p><p><strong>Constructors</strong></p><pre><code class="language-julia hljs">RandomFiniteGaussianMixture(x::AbstractVector{&lt;:Real}; kwargs...)
RandomFiniteGaussianMixture{T}(x::AbstractVector{&lt;:Real}; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>x</code>: The data vector.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>prior_components</code>: A dictionary containing the models with nonzero prior probabilities as keys and the corresponding prior probabilities (up to proportionality) as values. Defaults to <code>Dict(K =&gt; 1.0 for K in 1:20)</code>, corresponding to a uniform prior on the set {1, …, 20}.</li><li><code>prior_strength</code>: Strength parameter of the symmetric Dirichlet prior on the mixture weights. E.g. the prior is Dirichlet(strength, ..., strength). Defaults to <code>1.0</code>.</li><li><code>prior_location</code>: Prior mean of the location parameters <code>μ[k]</code>. Defaults to the midpoint of the minimum and maximum values in the sample.</li><li><code>prior_variance</code>: The prior variance of the location parameter <code>μ[k]</code>. Defaults to the sample range.</li><li><code>prior_shape</code>: Prior shape parameter of the squared scale parameters <code>σ2[k]</code>: Defaults to <code>2.0</code>.</li><li><code>hyperprior_shape</code>: Prior shape parameter of the hyperprior on the rate parameter of <code>σ2[k]</code>. Defaults to <code>0.2</code>.</li><li><code>hyperprior_rate</code>: Prior rate parameter of the hyperprior on the rate parameter of <code>σ2[k]</code>. Defaults to <code>0.2*R^2</code>, where <code>R</code> is the sample range.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);

julia&gt; rfgm = RandomFiniteGaussianMixture(x)
RandomFiniteGaussianMixture{Float64} with 20 values for the number mixture components.
Using 5000 observations.
Hyperparameters:
 prior_location = 0.5, prior_variance = 1.0
 prior_shape = 2.0, hyperprior_shape = 0.2, hyperprior_rate = 10.0
 prior_strength = 1.0

julia&gt; fgm = RandomFiniteGaussianMixture(x; prior_components = Dict(K =&gt; -log(K) for K in 1:10));</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L1-L36">source</a></section></details></article><h3 id="Evaluating-the-pdf-and-cdf"><a class="docs-heading-anchor" href="#Evaluating-the-pdf-and-cdf">Evaluating the pdf and cdf</a><a id="Evaluating-the-pdf-and-cdf-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluating-the-pdf-and-cdf" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="Distributions.pdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><a class="docstring-binding" href="#Distributions.pdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><code>Distributions.pdf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pdf(
    bsm::RandomFiniteGaussianMixture,
    params::NamedTuple,
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Union{Real, Vector{&lt;:Real}}

pdf(
    bsm::RandomFiniteGaussianMixture,
    params::AbstractVector{NamedTuple},
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Matrix{&lt;:Real}</code></pre><p>Evaluate <span>$f(t\, |\, \boldsymbol{\eta})$</span> for a given <code>RandomFiniteGaussianMixture</code> when the model parameters of the NamedTuple <code>params</code> are given by <span>$\boldsymbol{\eta}$</span>.</p><p>The named tuple should contain fields named <code>:μ</code>, <code>:σ2</code>, <code>:w</code> and optionally <code>:β</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L104-L120">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Distributions.cdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><a class="docstring-binding" href="#Distributions.cdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><code>Distributions.cdf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cdf(
    bsm::RandomFiniteGaussianMixture,
    params::NamedTuple,
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Union{Real, Vector{&lt;:Real}}

cdf(
    bsm::RandomFiniteGaussianMixture,
    params::AbstractVector{NamedTuple},
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Matrix{&lt;:Real}</code></pre><p>Evaluate <span>$F(t\, |\, \boldsymbol{\eta})$</span> for a given <code>RandomFiniteGaussianMixture</code> when the model parameters of the NamedTuple <code>params</code> are given by <span>$\boldsymbol{\eta}$</span>.</p><p>The named tuple should contain fields named <code>:μ</code>, <code>:σ2</code>, <code>:w</code> and optionally <code>:β</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L126-L142">source</a></section></details></article><h3 id="Utility-functions"><a class="docs-heading-anchor" href="#Utility-functions">Utility functions</a><a id="Utility-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-functions" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="BayesDensityCore.hyperparams-Tuple{RandomFiniteGaussianMixture}"><a class="docstring-binding" href="#BayesDensityCore.hyperparams-Tuple{RandomFiniteGaussianMixture}"><code>BayesDensityCore.hyperparams</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">hyperparams(
    gm::RandomFiniteGaussianMixture{T}
) where {T} -&gt; @NamedTuple{prior_strength::T, prior_location::T, prior_variance::T, prior_shape::T, prior_rate::T}</code></pre><p>Returns the hyperparameters of the finite Gaussian mixture model <code>gm</code> as a <code>NamedTuple</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L72-L78">source</a></section></details></article><h3 id="Variational-inference"><a class="docs-heading-anchor" href="#Variational-inference">Variational inference</a><a id="Variational-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Variational-inference" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior"><code>BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RandomFiniteGaussianMixtureVIPosterior{T&lt;:Real} &lt;: AbstractVIPosterior{T}</code></pre><p>Struct representing the variational posterior distribution of a <a href="#RandomFiniteGaussianMixture"><code>RandomFiniteGaussianMixture</code></a>.</p><p><strong>Fields</strong></p><ul><li><code>mixture_fits</code>: Dictionary consisting of 2-tuples, where the values are the posterior probability for a given <code>K</code> and the corresponding <a href="../FiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.FiniteGaussianMixtureVIPosterior"><code>FiniteGaussianMixtureVIPosterior</code></a>, containing the fitted variational posterior distributions for differing values of mixture components.</li><li><code>rgfm</code>: The <code>RandomFiniteGaussianMixture</code> to which the variational posterior was fit.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L1-L9">source</a></section></details></article><article><details class="docstring" open="true"><summary id="BayesDensityCore.varinf-Tuple{RandomFiniteGaussianMixture}"><a class="docstring-binding" href="#BayesDensityCore.varinf-Tuple{RandomFiniteGaussianMixture}"><code>BayesDensityCore.varinf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">varinf(
    rfgm::RandomFiniteGaussianMixture{T};
    max_iter::Int = 2000
    rtol::Real    = 1e-6
) where {T} -&gt; PitmanYorMixtureVIPosterior{T}</code></pre><p>Find a variational approximation to the posterior distribution of a <a href="#RandomFiniteGaussianMixture"><code>RandomFiniteGaussianMixture</code></a> using mean-field variational inference.</p><p><strong>Arguments</strong></p><ul><li><code>rfgm</code>: The <code>RandomFiniteGaussianMixture</code> whose posterior we want to approximate.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>max_iter</code>: Maximal number of VI iterations. Defaults to <code>2000</code>.</li><li><code>rtol</code>: Relative tolerance used to determine convergence. Defaults to <code>1e-6</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>vip</code>: A <a href="#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior"><code>RandomFiniteGaussianMixtureVIPosterior</code></a> object representing the variational posterior.</li><li><code>info</code>: A <a href="../../api/general_api/#BayesDensityCore.VariationalOptimizationResult"><code>VariationalOptimizationResult</code></a> describing the result of the optimization.</li></ul><div class="admonition is-info" id="Note-1c882b4669472036"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-1c882b4669472036" title="Permalink"></a></header><div class="admonition-body"><p>To perform the optimization for a fixed number of iterations irrespective of the convergence criterion, one can set <code>rtol = 0.0</code>, and <code>max_iter</code> equal to the desired total iteration count. Note that setting <code>rtol</code> to a strictly negative value will issue a warning.</p></div></div><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);

julia&gt; rfgm = RandomFiniteGaussianMixture(x);

julia&gt; vip = varinf(rfgm);

julia&gt; vip = varinf(rfgm; rtol=1e-7, max_iter=3000);</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L91-L127">source</a></section></details></article><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.posterior_prob_components"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.posterior_prob_components"><code>BayesDensityFiniteGaussianMixture.posterior_prob_components</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">posterior_prob_components(vip::RandomFiniteGaussianMixtureVIPosterior{T}) where {T} -&gt; Dict{Int, T}</code></pre><p>Get the variational posterior probability mass function of the number of mixture components as a dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L23-L27">source</a></section></details></article><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.maximum_a_posteriori"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.maximum_a_posteriori"><code>BayesDensityFiniteGaussianMixture.maximum_a_posteriori</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">maximum_a_posteriori(
    vip::RandomFiniteGaussianMixtureVIPosterior{T}
) where {T} -&gt; FiniteGaussianMixtureVIPosterior{T}</code></pre><p>Get the variational posterior distribution that maximizes the approximate posterior probability on the number of components q(K).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/11a662aca79970d484821db966f7225f54a20f49/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L30-L36">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../FiniteGaussianMixture/">« FiniteGaussianMixture</a><a class="docs-footer-nextpage" href="../../tutorials/add_new_models/">Implementing new Bayesian density estimators »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Tuesday 3 February 2026 07:44">Tuesday 3 February 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
