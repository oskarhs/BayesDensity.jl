<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RandomFiniteGaussianMixture · BayesDensity.jl</title><meta name="title" content="RandomFiniteGaussianMixture · BayesDensity.jl"/><meta property="og:title" content="RandomFiniteGaussianMixture · BayesDensity.jl"/><meta property="twitter:title" content="RandomFiniteGaussianMixture · BayesDensity.jl"/><meta name="description" content="Documentation for BayesDensity.jl."/><meta property="og:description" content="Documentation for BayesDensity.jl."/><meta property="twitter:description" content="Documentation for BayesDensity.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="BayesDensity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BayesDensity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../density_estimation_primer/">A primer on Bayesian nonparametric density estimation</a></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../api/general_api/">General API</a></li><li><a class="tocitem" href="../../api/plotting_api/">Plotting API</a></li></ul></li><li><span class="tocitem">Methods</span><ul><li><a class="tocitem" href="../">Index</a></li><li><a class="tocitem" href="../BSplineMixture/">BSplineMixture</a></li><li><a class="tocitem" href="../HistSmoother/">HistSmoother</a></li><li><a class="tocitem" href="../PitmanYorMixture/">PitmanYorMixture</a></li><li><a class="tocitem" href="../FiniteGaussianMixture/">FiniteGaussianMixture</a></li><li class="is-active"><a class="tocitem" href>RandomFiniteGaussianMixture</a><ul class="internal"><li><a class="tocitem" href="#Module-API"><span>Module API</span></a></li></ul></li><li><a class="tocitem" href="../RandomBernsteinPoly/">RandomBernsteinPoly</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/naive_bayes/">Naive Bayes</a></li><li><a class="tocitem" href="../../examples/model_selection/">Model selection</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/add_new_models/">Implementing new Bayesian density estimators</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Methods</a></li><li class="is-active"><a href>RandomFiniteGaussianMixture</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RandomFiniteGaussianMixture</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl/blob/main/docs/src/methods/RandomFiniteGaussianMixture.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="RandomFiniteGaussianMixture"><a class="docs-heading-anchor" href="#RandomFiniteGaussianMixture">RandomFiniteGaussianMixture</a><a id="RandomFiniteGaussianMixture-1"></a><a class="docs-heading-anchor-permalink" href="#RandomFiniteGaussianMixture" title="Permalink"></a></h1><p>Documentation for finite Gaussian mixture models, with a variable (random) number of mixture components.</p><p>This model is available through the <code>BayesDensityFiniteGaussianMixture</code> package.</p><p><code>RandomFiniteGaussianMixture</code> models the data-generating density as a mixture of normal distributions with an unknown number of mixture components. Our prior and model-specification follows that of <a href="../../references/#Richardson1997Mixtures">Richardson and Green (1997)</a>, and can be described as follows:</p><p class="math-container">\[\begin{align*}
x_i\,|\, K, \boldsymbol{w}, \boldsymbol{\mu}, \boldsymbol{\sigma}^2 &amp;\sim \sum_{k=1}^K \frac{w_k}{\sigma_k} \phi\Big(\frac{x_i - \mu_k}{\sigma_k}\Big), &amp;i = 1,\ldots, n,\\
w_k \,|\, K &amp;\sim \mathrm{Dirichlet}_K(\alpha, \ldots, \alpha),\\
\mu_k \,|\, K &amp;\sim \mathrm{Normal}(\mu_0, \sigma_0^2), &amp;k = 1,\ldots, K,\\
\sigma_k^2 \,|\, K, \beta &amp;\sim \mathrm{InverseGamma}(a_\sigma, \beta), &amp;k = 1,\ldots, K,\\
\beta &amp;\sim \mathrm{Gamma}(a_\beta, b_\beta),\\
K &amp;\sim p(K)
\end{align*}\]</p><p>where <span>$\phi(\cdot)$</span> denotes the density of the standard normal distribution, <span>$\mu_0 \in \mathbb{R}, \alpha, \sigma_0^2, a_\sigma, a_\beta, b_\beta &gt; 0$</span> are fixed hyperparameters and <span>$p(K)$</span> is a probability mass function supported on a finite subset of the positive integers.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1" class="footnote-ref">[1]</a><span class="footnote-preview" id="fn-1"></span></sup></p><p>This model is available through the <code>BayesDensityFiniteGaussianMixture</code> package.</p><p>For Markov chain Monte Carlo-based inference, we provide an implementation of the telescope sampler (<a href="../../references/#Fruhwirth2021Telescope">Frühwirth-Schnatter <em>et al.</em>, 2021</a>).</p><p>The variational inference algorithm used to compute the posterior first proceeds by separately fitting mixture models for different values of <span>$K$</span>, recording the corresponding value of the optimized evidence lower bound, <span>$\mathrm{ELBO}(K)$</span> at the end of each optimization. The posterior over the number of mixture components <span>$p(K\,|\, \boldsymbol{x})$</span> is then approximated via</p><p class="math-container">\[q(K) \propto p(K)\,\exp\big\{\mathrm{ELBO}(K)\big\}.\]</p><p>This approximation can be justified in light of the fact that the ELBO is a lower bound on the log-marginal likelihood <span>$p(\boldsymbol{x}\,|\, K)$</span>. The approximate posterior for the number of mixture components together with the optimal variational densities given <span>$K$</span> defines a distribution over a space of mixture of variable dimension, which is then used to make inferences about the density of the given sample.</p><p>The algorithm used to compute the conditional variational posterior <span>$q(\boldsymbol{\mu}|k)\,q(\boldsymbol{\sigma}^2|k)\,q(\boldsymbol{w}|k)$</span> is a variant of the algorithm 5 in <a href="../../references/#Ormerod2010explaining">Ormerod and Wand (2010)</a>. Note that our version also includes an additional hyperprior on the rate parameters of the mixture scales and that the algorithm has been adjusted to account for this fact.</p><p>There are two main ways of proceeding with Bayesian inference for the variational posterior. One possibility is to proceed with the single value <span>$\widehat{K}$</span> that maximizes the variational probability <span>$q(K)$</span>, the so-called maximum a posteriori model. Posterior inference then proceeds via the conditional variational posterior <span>$q\big(\boldsymbol{\mu}, \boldsymbol{\sigma}^2, \boldsymbol{w} | \widehat{K}\big)$</span>. This model can be retrieved by utilizing the <a href="#BayesDensityFiniteGaussianMixture.maximum_a_posteriori"><code>maximum_a_posteriori</code></a> method on a fitted variational posterior, which can then be used for posterior inference.</p><p>Another possibility is to take a fully Bayesian approach, where we do not condition on a single value of <span>$K$</span>, but treat it as a random variable. To pursure this approach to posterior inference, one can simply use the object returned by calling <a href="../../api/general_api/#BayesDensityCore.varinf-Tuple{AbstractBayesDensityModel}"><code>varinf</code></a> directly (e.g. for plotting or computing other posterior summary statistics).</p><h2 id="Module-API"><a class="docs-heading-anchor" href="#Module-API">Module API</a><a id="Module-API-1"></a><a class="docs-heading-anchor-permalink" href="#Module-API" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture"><code>BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RandomFiniteGaussianMixture{T&lt;:Real} &lt;: AbstractBayesDensityModel{T}</code></pre><p>Struct representing a finite Gaussian mixture model with a variable (random) number of components.</p><p><strong>Constructors</strong></p><pre><code class="language-julia hljs">RandomFiniteGaussianMixture(x::AbstractVector{&lt;:Real}; kwargs...)
RandomFiniteGaussianMixture{T}(x::AbstractVector{&lt;:Real}; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>x</code>: The data vector.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>prior_components</code>: A <a href="https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.DiscreteNonParametric"><code>Distributions.DiscreteNonParametric</code></a> distribution instance specifying the prior on the number of components <code>K</code>. Defaults to <code>DiscreteNonParametric(1:50, fill(T(1/50), 50))</code>, corresponding to a uniform prior on the set {1, …, 50}.</li><li><code>prior_strength</code>: Strength parameter of the symmetric Dirichlet prior on the mixture weights. E.g. the prior is Dirichlet(strength, ..., strength). Defaults to <code>1.0</code>.</li><li><code>prior_location</code>: Prior mean of the location parameters <code>μ[k]</code>. Defaults to the midpoint of the minimum and maximum values in the sample.</li><li><code>prior_variance</code>: The prior variance of the location parameter <code>μ[k]</code>. Defaults to the sample range.</li><li><code>prior_shape</code>: Prior shape parameter of the squared scale parameters <code>σ2[k]</code>: Defaults to <code>2.0</code>.</li><li><code>hyperprior_shape</code>: Prior shape parameter of the hyperprior on the rate parameter of <code>σ2[k]</code>. Defaults to <code>0.2</code>.</li><li><code>hyperprior_rate</code>: Prior rate parameter of the hyperprior on the rate parameter of <code>σ2[k]</code>. Defaults to <code>0.2*R^2</code>, where <code>R</code> is the sample range.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);

julia&gt; rfgm = RandomFiniteGaussianMixture(x)
RandomFiniteGaussianMixture{Float64} with 20 values for the number mixture components.
Using 5000 observations.
Hyperparameters:
 prior_location = 0.5, prior_variance = 1.0
 prior_shape = 2.0, hyperprior_shape = 0.2, hyperprior_rate = 10.0
 prior_strength = 1.0

julia&gt; rfgm = RandomFiniteGaussianMixture(x; prior_components = DiscreteNonParametric(1:12, fill(1/12, 12)));</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L1-L36">source</a></section></details></article><h3 id="Evaluating-the-pdf-and-cdf"><a class="docs-heading-anchor" href="#Evaluating-the-pdf-and-cdf">Evaluating the pdf and cdf</a><a id="Evaluating-the-pdf-and-cdf-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluating-the-pdf-and-cdf" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="Distributions.pdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><a class="docstring-binding" href="#Distributions.pdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><code>Distributions.pdf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">pdf(
    bsm::RandomFiniteGaussianMixture,
    params::NamedTuple,
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Union{Real, Vector{&lt;:Real}}

pdf(
    bsm::RandomFiniteGaussianMixture,
    params::AbstractVector{NamedTuple},
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Matrix{&lt;:Real}</code></pre><p>Evaluate <span>$f(t\, |\, \boldsymbol{\eta})$</span> for a given <code>RandomFiniteGaussianMixture</code> when the model parameters of the NamedTuple <code>params</code> are given by <span>$\boldsymbol{\eta}$</span>.</p><p>The named tuple should contain fields named <code>:μ</code>, <code>:σ2</code>, <code>:w</code> and optionally <code>:β</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L100-L116">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Distributions.cdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><a class="docstring-binding" href="#Distributions.cdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}"><code>Distributions.cdf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cdf(
    bsm::RandomFiniteGaussianMixture,
    params::NamedTuple,
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Union{Real, Vector{&lt;:Real}}

cdf(
    bsm::RandomFiniteGaussianMixture,
    params::AbstractVector{NamedTuple},
    t::Union{Real, AbstractVector{&lt;:Real}}
) -&gt; Matrix{&lt;:Real}</code></pre><p>Evaluate <span>$F(t\, |\, \boldsymbol{\eta})$</span> for a given <code>RandomFiniteGaussianMixture</code> when the model parameters of the NamedTuple <code>params</code> are given by <span>$\boldsymbol{\eta}$</span>.</p><p>The named tuple should contain fields named <code>:μ</code>, <code>:σ2</code>, <code>:w</code> and optionally <code>:β</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L122-L138">source</a></section></details></article><h3 id="Utility-functions"><a class="docs-heading-anchor" href="#Utility-functions">Utility functions</a><a id="Utility-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-functions" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="BayesDensityCore.hyperparams-Tuple{RandomFiniteGaussianMixture}"><a class="docstring-binding" href="#BayesDensityCore.hyperparams-Tuple{RandomFiniteGaussianMixture}"><code>BayesDensityCore.hyperparams</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">hyperparams(
    gm::RandomFiniteGaussianMixture{T}
) where {T} -&gt; @NamedTuple{prior_components::DiscreteNonParametric{Int, T}, prior_strength::T, prior_location::T, prior_variance::T, prior_shape::T, prior_rate::T}</code></pre><p>Returns the hyperparameters of the finite Gaussian mixture model <code>gm</code> as a <code>NamedTuple</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/RandomFiniteGaussianMixture.jl#L69-L75">source</a></section></details></article><h3 id="Markov-chain-Monte-Carlo"><a class="docs-heading-anchor" href="#Markov-chain-Monte-Carlo">Markov chain Monte Carlo</a><a id="Markov-chain-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Markov-chain-Monte-Carlo" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="StatsBase.sample-Tuple{AbstractRNG, RandomFiniteGaussianMixture, Int64}"><a class="docstring-binding" href="#StatsBase.sample-Tuple{AbstractRNG, RandomFiniteGaussianMixture, Int64}"><code>StatsBase.sample</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">sample(
    [rng::Random.AbstractRNG],
    rfgm::RandomFiniteGaussianMixture{T},
    n_samples::Int;
    n_burnin::Int              = min(1000, div(n_samples, 5)),
    initial_params::NamedTuple = _get_default_initparams_mcmc(rfgm)
) where {T} -&gt; PosteriorSamples{T}</code></pre><p>Generate <code>n_samples</code> posterior samples from a <code>RandomFiniteGaussianMixture</code> using the telescope sampler.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Optional random seed used for random variate generation.</li><li><code>rfgm</code>: The <code>RandomFiniteGaussianMixture</code> object for which posterior samples are generated.</li><li><code>n_samples</code>: The total number of samples (including burn-in).</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>n_burnin</code>: Number of burn-in samples.</li><li><code>initial_params</code>: Initial values used in the MCMC algorithm. Should be supplied as a <code>NamedTuple</code> with fields <code>:μ</code>, <code>:σ2</code> and <code>:w</code>, where all are <code>K</code>-dimensional vectors.</li></ul><p>The following constraints must also be satisfied: <code>σ2[k]&gt;0</code> for all <code>k</code> and <code>w[k]≥0</code> for all <code>k</code> and <code>sum(w) ≈ 1</code></p><p><strong>Returns</strong></p><ul><li><code>ps</code>: A <a href="../../api/general_api/#BayesDensityCore.PosteriorSamples"><code>PosteriorSamples</code></a> object holding the posterior samples and the original model object.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);

julia&gt; rfgm = RandomFiniteGaussianMixture(x);

julia&gt; ps1 = sample(rfgm, 5_000);

julia&gt; ps2 = sample(rfgm, 5_000; n_burnin=2_000, initial_params = (μ = [0.2, 0.8], σ2 = [1.0, 2.0], w = [0.7, 0.3]));</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/mcmc.jl#L1-L37">source</a></section></details></article><h3 id="Variational-inference"><a class="docs-heading-anchor" href="#Variational-inference">Variational inference</a><a id="Variational-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Variational-inference" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior"><code>BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RandomFiniteGaussianMixtureVIPosterior{T&lt;:Real} &lt;: AbstractVIPosterior{T}</code></pre><p>Struct representing the variational posterior distribution of a <a href="#RandomFiniteGaussianMixture"><code>RandomFiniteGaussianMixture</code></a>.</p><p><strong>Fields</strong></p><ul><li><code>posterior_components</code>: The posterior probabilities on the number of components. Note that <code>support(posterior_components)[K]</code> corresponds to the posterior probability of model <code>K</code>.</li><li><code>mixture_fits</code>: Vector of <a href="../FiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.FiniteGaussianMixtureVIPosterior"><code>FiniteGaussianMixtureVIPosterior</code></a> objects, containing the fitted variational posterior distributions for differing values of mixture components.</li><li><code>rgfm</code>: The <code>RandomFiniteGaussianMixture</code> to which the variational posterior was fit.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L1-L10">source</a></section></details></article><article><details class="docstring" open="true"><summary id="BayesDensityCore.varinf-Tuple{RandomFiniteGaussianMixture}"><a class="docstring-binding" href="#BayesDensityCore.varinf-Tuple{RandomFiniteGaussianMixture}"><code>BayesDensityCore.varinf</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">varinf(
    rfgm::RandomFiniteGaussianMixture{T};
    max_iter::Int = 2000
    rtol::Real    = 1e-6
) where {T} -&gt; PitmanYorMixtureVIPosterior{T}</code></pre><p>Find a variational approximation to the posterior distribution of a <a href="#RandomFiniteGaussianMixture"><code>RandomFiniteGaussianMixture</code></a> using mean-field variational inference.</p><p><strong>Arguments</strong></p><ul><li><code>rfgm</code>: The <code>RandomFiniteGaussianMixture</code> whose posterior we want to approximate.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>max_iter</code>: Maximal number of VI iterations. Defaults to <code>2000</code>.</li><li><code>rtol</code>: Relative tolerance used to determine convergence. Defaults to <code>1e-6</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>vip</code>: A <a href="#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior"><code>RandomFiniteGaussianMixtureVIPosterior</code></a> object representing the variational posterior.</li><li><code>info</code>: A <a href="../../api/general_api/#BayesDensityCore.VariationalOptimizationResult"><code>VariationalOptimizationResult</code></a> describing the result of the optimization.</li></ul><div class="admonition is-info" id="Note-1c882b4669472036"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-1c882b4669472036" title="Permalink"></a></header><div class="admonition-body"><p>To perform the optimization for a fixed number of iterations irrespective of the convergence criterion, one can set <code>rtol = 0.0</code>, and <code>max_iter</code> equal to the desired total iteration count. Note that setting <code>rtol</code> to a strictly negative value will issue a warning.</p></div></div><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);

julia&gt; rfgm = RandomFiniteGaussianMixture(x);

julia&gt; vip = varinf(rfgm);

julia&gt; vip = varinf(rfgm; rtol=1e-7, max_iter=3000);</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L77-L113">source</a></section></details></article><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.posterior_components"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.posterior_components"><code>BayesDensityFiniteGaussianMixture.posterior_components</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">posterior_components(vip::RandomFiniteGaussianMixtureVIPosterior{T}) where {T} -&gt; DiscreteNonParametric{Int, T}</code></pre><p>Get the variational posterior probability mass function of the number of mixture components as a <code>DiscreteNonParametric</code> instance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L26-L30">source</a></section></details></article><article><details class="docstring" open="true"><summary id="BayesDensityFiniteGaussianMixture.maximum_a_posteriori"><a class="docstring-binding" href="#BayesDensityFiniteGaussianMixture.maximum_a_posteriori"><code>BayesDensityFiniteGaussianMixture.maximum_a_posteriori</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">maximum_a_posteriori(
    vip::RandomFiniteGaussianMixtureVIPosterior{T}
) where {T} -&gt; FiniteGaussianMixtureVIPosterior{T}</code></pre><p>Get the variational posterior distribution that maximizes the approximate posterior probability on the number of components q(K).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/oskarhs/BayesDensity.jl/blob/e3c8f83dbbaab5bf1c889436d20905e3cfbbfb08/lib/BayesDensityFiniteGaussianMixture/src/RandomFiniteGaussianMixture/variational.jl#L33-L39">source</a></section></details></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>We use the rate parameterization of the <a href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma distribution</a> here. This differs from the scale-parameterization used by <code>Distributions</code>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../FiniteGaussianMixture/">« FiniteGaussianMixture</a><a class="docs-footer-nextpage" href="../RandomBernsteinPoly/">RandomBernsteinPoly »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 12 February 2026 16:01">Thursday 12 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
