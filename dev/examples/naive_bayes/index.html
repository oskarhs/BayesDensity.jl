<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Naive Bayes · BayesDensity.jl</title><meta name="title" content="Naive Bayes · BayesDensity.jl"/><meta property="og:title" content="Naive Bayes · BayesDensity.jl"/><meta property="twitter:title" content="Naive Bayes · BayesDensity.jl"/><meta name="description" content="Documentation for BayesDensity.jl."/><meta property="og:description" content="Documentation for BayesDensity.jl."/><meta property="twitter:description" content="Documentation for BayesDensity.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="BayesDensity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BayesDensity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../density_estimation_primer/">A primer on Bayesian nonparametric density estimation</a></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../api/general_api/">General API</a></li><li><a class="tocitem" href="../../api/plotting_api/">Plotting API</a></li></ul></li><li><span class="tocitem">Methods</span><ul><li><a class="tocitem" href="../../methods/">Index</a></li><li><a class="tocitem" href="../../methods/BSplineMixture/">BSplineMixture</a></li><li><a class="tocitem" href="../../methods/HistSmoother/">HistSmoother</a></li><li><a class="tocitem" href="../../methods/PitmanYorMixture/">PitmanYorMixture</a></li><li><a class="tocitem" href="../../methods/FiniteGaussianMixture/">FiniteGaussianMixture</a></li><li><a class="tocitem" href="../../methods/RandomFiniteGaussianMixture/">RandomFiniteGaussianMixture</a></li><li><a class="tocitem" href="../../methods/RandomBernsteinPoly/">RandomBernsteinPoly</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Naive Bayes</a><ul class="internal"><li><a class="tocitem" href="#A-real-data-example"><span>A real-data example</span></a></li></ul></li><li><a class="tocitem" href="../model_selection/">Model selection</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/add_new_models/">Implementing new Bayesian density estimators</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Naive Bayes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Naive Bayes</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/oskarhs/BayesDensity.jl/blob/main/docs/src/examples/naive_bayes.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Naive-Bayes"><a class="docs-heading-anchor" href="#Naive-Bayes">Naive Bayes</a><a id="Naive-Bayes-1"></a><a class="docs-heading-anchor-permalink" href="#Naive-Bayes" title="Permalink"></a></h1><p>This example illustrates how <code>BayesDensity</code> can be used for classification tasks as part of a naive Bayes classifier. Suppose that we observe a set of binary outcome variables <span>$C_i \in \{0, 1\}$</span> for <span>$i=1,\ldots, n$</span>, and that we are interested in classifying each observation based on a set of <span>$d$</span>-dimensional covariates <span>$\boldsymbol{x}_i = (x_{i,1}, \ldots, x_{i,d})^\top$</span>. One possible approach to this classification task is to assume a generative model for the class membership, where the observations have been generated according to density <span>$f_0$</span> if <span>$C_i = 0$</span> and density <span>$f_1$</span> if <span>$C_i = 1$</span>. The probability of belonging to a given class conditional on the values of the observed covariates can then be computed via Bayes&#39; theorem,</p><p class="math-container">\[P(C_i = c\,|\, \boldsymbol{x}_i) \propto P(C_i = c)\, f_c(\boldsymbol{x}_i), \quad c = 0, 1.\]</p><p>The Bayes classifier in the above setting is to predict that each <span>$C_i$</span> is equal to the class which maximizes the posterior probability. Both the prior class probabilities <span>$P(C_i = c)$</span> and the class-specific densities <span>$f_0$</span> and <span>$f_1$</span> are in general not known and need to be estimated from the observed sample. A simple estimate of the prior class probabilities are the corresponding sample proportions, while the class-specific densities can be fit via either parametric or nonparametric methods.</p><p>Assuming a nonparametric model for the densities <span>$f_c$</span> immediately stands out as an attractive option in this case, as one does not have to postulate a parametric family of distributions to which the component densities belong. However, in cases where the dimension of the covariate vector is moderate relative to the number of observations, nonparametric density estimation is notoriously difficult, a phenomenon often referred to as <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">the curse of dimensionality</a>. A solution to this issue is to use a so-called <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">naive Bayes classifier</a>. This class of classifiers makes the simplifying assumption that the densities of the features <span>$x_j$</span> are independent conditional on the class membership, i.e. we can write  <span>$f_c(\boldsymbol{x}_i) = \prod_{j=1}^d f_{c, j}(x_{i,j})$</span>. Under this assumption, the problem of fitting the class-specific densities <span>$f_c$</span> reduces to that of fitting multiple univariate density estimators. Although the independence assumption is often unrealistic in practice, it often ends up performing better than the alternative due to the inefficiency of nonparametric density estimation in higher dimensions.</p><h2 id="A-real-data-example"><a class="docs-heading-anchor" href="#A-real-data-example">A real-data example</a><a id="A-real-data-example-1"></a><a class="docs-heading-anchor-permalink" href="#A-real-data-example" title="Permalink"></a></h2><p>To illustrate the naive Bayes classifier in practice, we consider the Pima indians dataset from the MASS R package, which is easily available through <code>RDatasets.jl</code>. This dataset consists of diabetes measurements from a population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona. The dataset also includes a total of <span>$7$</span> numeric covariates, which we will use to predict whether or not the diabetes test of a given individual is positive. In order to evaluate the out-of-sample performance of our classifier, we split the dataset into a training and a test set. The version of the data set that we will be using here contains missing values, which is a problem when attempting to estimate the joint density of the covariates. In contrast, this does not pose a problem for naive Bayes since the marginal densities are estimated separately, and we therefore do not have to discard a data point <span>$x_{i,j}$</span> when estimating the density <span>$f_{c,j}$</span> when the value of <span>$x_{i,l}$</span> is missing. Due to the fact that there are only <span>$68$</span> individuals with a positive outcome in the training dataset for which we have complete data, a fully nonparametric approach to estimating <span>$f_0$</span> and <span>$f_1$</span> is unlikely to work well. Here, we fit a <a href="../../methods/BSplineMixture/#BSplineMixture"><code>BSplineMixture</code></a> model to the marginals <span>$f_{c,j}$</span> of the component densities, and use the resulting posterior medians as point estimates of <span>$f_{c,j}(x_{i,j})$</span>.</p><pre><code class="language-julia hljs">using BayesDensityBSplineMixture
using DataFrames
using Random
using RDatasets

# Set seed
rng = Xoshiro(1984)

# Load train and test datasets
train = dataset(&quot;MASS&quot;, &quot;Pima.tr2&quot;) # NB! This data set contains some missing values
test = dataset(&quot;MASS&quot;, &quot;Pima.te&quot;)

# Split the training dataset into positive and negative observations
train_positive = train[train.Type .== &quot;Yes&quot;, :]
train_negative = train[train.Type .== &quot;No&quot;, :]

# Compute sample proportions
prior_positive = nrow(train_positive) / nrow(train)
prior_negative = nrow(train_negative) / nrow(train)

# Fit univariate density estimates to each covariate
predictor_names = [:NPreg, :Glu, :BP, :Skin, :BMI, :Ped, :Age]
densities_positive = []
densities_negative = []

for predictor in predictor_names
    # Filter missing values before estimating the densities:
    x_positive_filtered = collect(skipmissing(train_positive[:,predictor]))
    x_negative_filtered = collect(skipmissing(train_negative[:,predictor]))
    fit_positive = sample(
        rng,
        BSplineMixture(x_positive_filtered; prior_global_rate=1e-4),
        10_000
    )
    fit_negative = sample(
        rng,
        BSplineMixture(x_negative_filtered; prior_global_rate=1e-4),
        10_000
    )
    push!(densities_positive, fit_positive)
    push!(densities_negative, fit_negative)
end

# Compute predictions:
predicted_class = Vector{String}(undef, nrow(test))
predicted_proba = Vector{Float64}(undef, nrow(test)) # Prob of positive result
for i in 1:nrow(test)
    posterior_positive = prior_positive
    posterior_negative = prior_negative
    for j in eachindex(predictor_names)
        posterior_positive *= median(densities_positive[j], test[i, predictor_names[j]])
        posterior_negative *= median(densities_negative[j], test[i, predictor_names[j]])
    end
    predicted_proba[i] = posterior_positive / (posterior_positive + posterior_negative)
    predicted_class[i] = ifelse(posterior_positive &gt; posterior_negative, &quot;Yes&quot;, &quot;No&quot;)
end

# Compute accuracies:
acc = mean(test.Type .== predicted_class)</code></pre><p>The resulting accuracy on the test set is <span>$0.789$</span>. This is a significant improvement over the baseline classifier that always predicts a negative result (i.e. the majority class in the training set), which here resulted in a test set accuracy of <span>$0.672$</span>, showing that the naive Bayes classifier improves significantly over the simple majority classification rule. For comparison, we also fitted a parametric naive Bayes classifier to the same training dataset, where the marginal distributions are assumed to be normal, which is the original naive Bayes approach (<a href="../../references/#Hastie2009Elements">Hastie <em>et al.</em>, 2009</a>). This yielded a test set accuracy of <span>$0.783$</span>, slightly lower than that obtained with our nonparametric approach.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../methods/RandomBernsteinPoly/">« RandomBernsteinPoly</a><a class="docs-footer-nextpage" href="../model_selection/">Model selection »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Sunday 15 February 2026 17:20">Sunday 15 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
