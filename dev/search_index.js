var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"Bishop, C. (2006). Pattern Recognition and Machine Learning. First Edition (Springer New York, NY).\n\n\n\nBlei, D. M. and Jordan, M. I. (2006). Variational inference for Dirichlet process mixtures. Bayesian Analysis 1, 121–143.\n\n\n\nGelman, A.; Carlin, J.; Stern, H.; Dunson, D.; Vehtari, A. and Rubin, D. (2013). Bayesian Data Analysis. Third Edition (Chapman and Hall/CRC).\n\n\n\nIshwaran, H. and James, L. (2001). Gibbs sampling methods for stick-breaking priors. Journal of the American statistical Association 96, 161–173.\n\n\n\nNeal, R. M. (2000). Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics 9, 249–265.\n\n\n\nOrmerod, J. and Wand, M. (2010). Explaining variational approximations. The American Statistician 64, 140–153.\n\n\n\nPetrone, S. (1999). Bayesian density estimation using Bernstein polynomials. Canadian Journal of Statistics 27, 105–126.\n\n\n\nRichardson, S. and Green, P. (1997). On Bayesian Analysis of Mixtures with an Unknown Number of Components. Journal of the Royal Statistical Society Series B: Statistical Methodology 59.\n\n\n\nScott, D. (1992). Multivariate Density Estimation: Theory, Practice, and Visualization (Wiley).\n\n\n\nWand, M. and Yu, J. (2022). Density estimation via Bayesian inference engines. AStA Advances in Statistical Analysis 106, 199–216.\n\n\n\n","category":"section"},{"location":"density_estimation_primer/#A-primer-on-Bayesian-nonparametric-density-estimation","page":"A primer on Bayesian nonparametric density estimation","title":"A primer on Bayesian nonparametric density estimation","text":"The following page provides a short introduction to nonparametric density estimation, with a particular focus on Bayesian approaches. The topic of nonparametric density estimation is vast, and we do as such only aim to provide a rather broad overview here. For an in-depth and quite accessible introduction to frequentist approaches in nonparametric density estimation, we recommend Scott (1992). A good introduction to the Bayesian perspective, with a focus on practical computation, can be found in Gelman et al. (2013).","category":"section"},{"location":"density_estimation_primer/#Introduction","page":"A primer on Bayesian nonparametric density estimation","title":"Introduction","text":"The goal of density estimation is to estimate an unknown density based on observed data. Given an independent and identically distributed sample x_1 x_2 ldots x_n from a density f, the goal is to construct an estimate hatf that is close to the true density. Parametric approaches to density estimation assume that the density belongs to a parametric family of distribution, such as the normal family of distributions, indexed by a parameter of low to moderate dimension. This approach is generally highly efficient if the true density belongs to or is well-approximated by a member of the postulated parametric family. However, in cases where the parametric family at hand is misspecified, the quality of the estimator hatf is often of low quality.\n\nIn constrast, nonparametric approaches to density estimation strive to make less restrictive assumptions on the true density f. This aim is acheived by employing richer classes of models which are able to approximate the true density under minimal smoothness assumptions. Unlike their parametric counterparts, most nonparametric density estimators are built on high- or even infinite-dimensional parameter spaces.\n\nAlthough the increased flexibility of nonparametric approaches is an attractive feature, it can be quite challenging to construct well-working nonparametric density estimators owing to the high-dimensional parameter spaces. In particular, the high dimensionality of the parameter space introduces the need for some form of regularization, typically through the introduction of one or several smoothing parameters, so that the resulting estimates do not end up being to bumpy. As an illustration, consider the well-known (frequentist) kernel density estimator:\n\nhatf(x) = frac1nhsum_i=1^n phiBig(fracx - x_ihBig)\n\nwhere phi(cdot) is the probability density function of the standard normal distribution and the bandwidth h  0 is a parameter which controls the smoothness of the estimate. To illustrate the role of the smoothing parameter, we generated a sample of size n = 1000 from the following mixture density\n\nf(x) = frac04025 phiBig(fracx+02025Big) + frac06015phiBig(fracx-05015Big)\n\nWe then fitted a kernel density estimator for three different choices of the bandwidth parameter. The resulting density estimates are shown in the figure below:\n\n(Image: kernel density bandwidth)\n\nFor the smallest choice of the bandwidth parameter, the estimate hatf is very wiggly, severly distorting the shape of f with many small bumps. On the other hand, the largest bandwidth results in far too much smoothing, and the resulting density estimate fails to resemble f. A good choice of the bandwidth parameter needs to strike a balance between these two extremes.\n\nIn practical applications, the true density is of course not known, and one has to select the smoothing parameter in a data-driven manner. Frequentist approaches to selecting the smoothing parameter are often based on optimizing some criterion that balances representational capacity against model complexity. Examples include the optimization of a cross-validation criterion or the minimization of an asymptotic expansion of a risk function.","category":"section"},{"location":"density_estimation_primer/#The-Bayesian-approach","page":"A primer on Bayesian nonparametric density estimation","title":"The Bayesian approach","text":"Bayesian statistics offers a fundamentally different approach to nonparametric density estimation. Here, the density f is itself treated as a random quantity, and is assigned a prior distribution p(f). Density estimates in Bayesian models are based on the posterior distribution,\n\np(f x_1 ldots x_n) propto p(f)prod_i=1^n f(x_i)\n\nUnlike frequentist approaches to smoothing parameter selection, where they are typically chosen via an optimization, the Bayesian approach implicitly introduces smoothing through the prior distribution. By utilizing a prior distribution that allows for f to be of varying degrees of smoothness, Bayesian procedures lead to data-driven choices to smoothing parameter(s) through the posterior distribution p(f x_1 ldots x_n). Although the posterior gives a full distribution over the density f, statisticians are often interested in providing a point estimate hatf(x), along with some uncertainty measure. A popular point estimate is the posterior mean, hatf(x) = mathbbEf(x) x_1 ldots x_n, while uncertainty quantification is usually provided in the form of a 95  credible band, which is constructed by finding two functions l and u which satisfy P(l(x)leq f(x) leq u(x) x_1 ldots x_n) = 095 for all x. A key challenge in the Bayesian approach to density estimation is that for most genuinely nonparametric models, the posterior distribution p(f x_1 ldots x_n) is often not tractable analytically, and as such, posterior summaries cannot be computed exactly. Instead, inference is mostly based on Monte Carlo approximations, where one generates samples f^(1) ldots f^(M) from the posterior distribution of f, and uses them to approximate key posterior quantities. For instance, the posterior mean can be approximated by\n\n    mathbbEf(x) x_1 ldots x_n approx frac1Msum_m=1^M f^(m)(x)\n\nwhile approximate posterior credible bands can be constructed via the sample quantiles of f(x).\n\nThe key challenge to constructing successful Bayesian nonparametric density estimators is to find a model that not only has a very large representational capacity, a prior distribution that results in smooth density estimates and a combination of prior and model for which efficient approximate posterior inference is possible.","category":"section"},{"location":"density_estimation_primer/#Markov-chain-Monte-Carlo","page":"A primer on Bayesian nonparametric density estimation","title":"Markov chain Monte Carlo","text":"Although the Monte Carlo method is a powerful technique for approximating key quantities of interest, the process of generating samples from the posterior distribution is less straightforward. Moreover, the high- to infinite dimensional nature of nonparametric procedures further complicate this issue, as the techniques employed for approximate inference in lower-dimensional Bayesian models often do not scale well to higher dimensions. The Bayesian methods used to generate samples from the posterior distribution in the BayesDensity package are so-called Markov chain Monte Carlo (MCMC) methods, where one constructs a Markov chain f^(1) ldots f^(M), where the distribution of f^(m+1) is in general dependent on the previous state f^(m). Although the sequence of densities generated from such a sampler are dependent, it can be shown under rather mild conditions that Monte Carlo approximations based on these samples approach their corresponding true values as the number of samples diverges to infinity.\n\nTo illustrate the appliation of Markov chain Monte Carlo methods to Bayesian density estimation, we applied the implemented MCMC sampler to a HistSmoother model with default hyperparameters based on the simulated sample from the two-component Gaussian mixture treated previously. The animation shown below displays the posterior draw f^(m) for the first 200 iterations of the sampler, along with the runnning mean.\n\n(Image: MCMC animation)\n\nFor the first few draws, the samples f^(m) do not resemble the true density much due to the dependence on the initial value f^(0).[1] However, it only takes the draws a few iterations to reach a stady state around which the remaining posterior draws fluctuate quickly. While there is considerable variation in the posterior draws of f after reaching the stationary distribution, the running mean of the draws stabilizes after a while, resulting in a density estimate which is quite close to the true data-generating density in this case.\n\n[1]: The behavior observed in the animation, that the first few values of a Markov chain are not properly representative draws from the posterior distribution, is rather common when using Markov chain Monte Carlo methods. A common remedy to this fact is to use a so-called burn-in period, where one discards the samples from the first iterations of the sampler, in order to minimize any bias in posterior estimates caused by the initial values.\n\nTo estimate the true density, we ran the sampler for an additional 1900 iterations, and discarded the samples from the first 100 iterations to reduce the initialization bias in the estimates. The resulting approximations to the posterior mean and the 95  pointwise credible bands are shown in the figure below.\n\n(Image: MCMC estimate)","category":"section"},{"location":"density_estimation_primer/#Variational-inference","page":"A primer on Bayesian nonparametric density estimation","title":"Variational inference","text":"Variational inference (VI) is a designation used to describe a class of methods that construct deterministic approximations to posterior distributions. The main benefit of the variational approach over Markov chain Monte Carlo methods is that the approximations to key posterior quantities are often of high quality, and that they are significantly faster to construct than generating a sufficient number of posterior draws via MCMC. Mathematically, variational approaches seek to find an approximation q(f) to the posterior p(f x_1 ldots x_n), where the density q belongs to a family of distributions that is easy to sample from. Most variational approaches are based on minimizing the Kullback–Leibler divergence between q and the posterior, as this objective has certain mathematical properties that make it especially suitable for numerical optimization techniques. Having fitted a variational posterior, we can apply the Monte Carlo method to independent samples f^(m)sim q(f) to compute e.g. the posterior mean.[2] Unlike estimates based on Markov chain Monte Carlo, where true posterior quantities can be recovered in the limit as the number of samples approaches infinity, variational approximations induce some irreducible error due to their approximate nature.\n\n[2]: Analytical expressions for posterior means and credible intervals for f are often not available under q.\n\nTo illustrate the application of variational methods to density estimation, we return to the two-component normal mixture data considered previously. We have fitted a variational approximation to the same HistSmoother considered previously. The plot below shows the resulting approximate posterior mean and the 95  pointwise credible bands.\n\n(Image: VI estimate)\n\nFor this particular example, the estimates obtained from the variational approximation are in very close agreement to those obtained via MCMC. Note that the quality of the variational approximation is in general highly model-dependent, and you should not necessarily expect to see such close agreement between posterior draws via MCMC and the approximate VI posterior for all the models implemented in BayesDensity.","category":"section"},{"location":"methods/RandomFiniteGaussianMixture/#RandomFiniteGaussianMixture","page":"RandomFiniteGaussianMixture","title":"RandomFiniteGaussianMixture","text":"Documentation for finite Gaussian mixture models, with a variable (random) number of mixture components.\n\nThis model is available through the BayesDensityFiniteGaussianMixture package.\n\nThe variational inference algorithm used to compute the posterior first proceeds by separately fitting mixture models for different values of K, recording the corresponding value of the optimized evidence lower bound, mathrmELBO(K) at the end of each optimization. The posterior over the number of mixture components p(K boldsymbolx) is then approximated via\n\nq(K) propto p(K)expbigmathrmELBO(K)big\n\nThis approximation can be justified in light of the fact that the ELBO is a lower bound on the log-marginal likelihood p(boldsymbolx K). The approximate posterior for the number of mixture components together with the optimal variational densities given K defines a distribution over a space of mixture of variable dimension, which is then used to make inferences about the density of the given sample.\n\nThe algorithm used to compute the conditional variational posterior q(boldsymbolmuk)q(boldsymbolsigma^2k)q(boldsymbolwk) is a variant of the algorithm 5 in Ormerod and Wand (2010). Note that our version also includes an additional hyperprior on the rate parameters of the mixture scales and that the algorithm has been adjusted to account for this fact.\n\nThere are two main ways of proceeding with Bayesian inference for the variational posterior. One possibility is to proceed with the single value hatK that maximizes the variational probability q(K), the so-called maximum a posteriori model. Posterior inference then proceeds via the conditional variational posterior qbig(boldsymbolmu boldsymbolsigma^2 boldsymbolw  hatKbig). This model can be retrieved by utilizing the maximum_a_posteriori method on a fitted variational posterior, which can then be used for posterior inference.\n\nAnother possibility is to take a fully Bayesian approach, where we do not condition on a single value of K, but treat it as a random variable. To pursure this approach to posterior inference, one can simply use the object returned by calling varinf directly (e.g. for plotting or computing other posterior summary statistics).","category":"section"},{"location":"methods/RandomFiniteGaussianMixture/#Module-API","page":"RandomFiniteGaussianMixture","title":"Module API","text":"","category":"section"},{"location":"methods/RandomFiniteGaussianMixture/#Evaluating-the-pdf-and-cdf","page":"RandomFiniteGaussianMixture","title":"Evaluating the pdf and cdf","text":"","category":"section"},{"location":"methods/RandomFiniteGaussianMixture/#Utility-functions","page":"RandomFiniteGaussianMixture","title":"Utility functions","text":"","category":"section"},{"location":"methods/RandomFiniteGaussianMixture/#Variational-inference","page":"RandomFiniteGaussianMixture","title":"Variational inference","text":"","category":"section"},{"location":"methods/RandomFiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture","page":"RandomFiniteGaussianMixture","title":"BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixture","text":"RandomFiniteGaussianMixture{T<:Real} <: AbstractBayesDensityModel{T}\n\nStruct representing a finite Gaussian mixture model with a variable (random) number of components.\n\nConstructors\n\nRandomFiniteGaussianMixture(x::AbstractVector{<:Real}; kwargs...)\nRandomFiniteGaussianMixture{T}(x::AbstractVector{<:Real}; kwargs...)\n\nArguments\n\nx: The data vector.\n\nKeyword arguments\n\nprior_components: A DiscreteNonParametric distribution containing the models with nonzero prior probabilities as keys and the corresponding prior probabilities (up to proportionality) as values. Defaults to DiscreteNonParametric(1:20, fill(T(1/20), 20)), corresponding to a uniform prior on the set {1, …, 20}.\nprior_strength: Strength parameter of the symmetric Dirichlet prior on the mixture weights. E.g. the prior is Dirichlet(strength, ..., strength). Defaults to 1.0.\nprior_location: Prior mean of the location parameters μ[k]. Defaults to the midpoint of the minimum and maximum values in the sample.\nprior_variance: The prior variance of the location parameter μ[k]. Defaults to the sample range.\nprior_shape: Prior shape parameter of the squared scale parameters σ2[k]: Defaults to 2.0.\nhyperprior_shape: Prior shape parameter of the hyperprior on the rate parameter of σ2[k]. Defaults to 0.2.\nhyperprior_rate: Prior rate parameter of the hyperprior on the rate parameter of σ2[k]. Defaults to 0.2*R^2, where R is the sample range.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> rfgm = RandomFiniteGaussianMixture(x)\nRandomFiniteGaussianMixture{Float64} with 20 values for the number mixture components.\nUsing 5000 observations.\nHyperparameters:\n prior_location = 0.5, prior_variance = 1.0\n prior_shape = 2.0, hyperprior_shape = 0.2, hyperprior_rate = 10.0\n prior_strength = 1.0\n\njulia> fgm = RandomFiniteGaussianMixture(x; prior_components = DiscreteNonParametric(1:12, fill(1/12)));\n\n\n\n\n\n","category":"type"},{"location":"methods/RandomFiniteGaussianMixture/#Distributions.pdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}","page":"RandomFiniteGaussianMixture","title":"Distributions.pdf","text":"pdf(\n    bsm::RandomFiniteGaussianMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\npdf(\n    bsm::RandomFiniteGaussianMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate f(t  boldsymboleta) for a given RandomFiniteGaussianMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain fields named :μ, :σ2, :w and optionally :β.\n\n\n\n\n\n","category":"method"},{"location":"methods/RandomFiniteGaussianMixture/#Distributions.cdf-Tuple{RandomFiniteGaussianMixture, NamedTuple, Real}","page":"RandomFiniteGaussianMixture","title":"Distributions.cdf","text":"cdf(\n    bsm::RandomFiniteGaussianMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\ncdf(\n    bsm::RandomFiniteGaussianMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate F(t  boldsymboleta) for a given RandomFiniteGaussianMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain fields named :μ, :σ2, :w and optionally :β.\n\n\n\n\n\n","category":"method"},{"location":"methods/RandomFiniteGaussianMixture/#BayesDensityCore.hyperparams-Tuple{RandomFiniteGaussianMixture}","page":"RandomFiniteGaussianMixture","title":"BayesDensityCore.hyperparams","text":"hyperparams(\n    gm::RandomFiniteGaussianMixture{T}\n) where {T} -> @NamedTuple{prior_components::DiscreteNonParametric{Int, T}, prior_strength::T, prior_location::T, prior_variance::T, prior_shape::T, prior_rate::T}\n\nReturns the hyperparameters of the finite Gaussian mixture model gm as a NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"methods/RandomFiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior","page":"RandomFiniteGaussianMixture","title":"BayesDensityFiniteGaussianMixture.RandomFiniteGaussianMixtureVIPosterior","text":"RandomFiniteGaussianMixtureVIPosterior{T<:Real} <: AbstractVIPosterior{T}\n\nStruct representing the variational posterior distribution of a RandomFiniteGaussianMixture.\n\nFields\n\nposterior_components: The posterior probabilities on the number of components. Note that support(posterior_components)[K] corresponds to the posterior probability of model K.\nmixture_fits: Vector of FiniteGaussianMixtureVIPosterior objects, containing the fitted variational posterior distributions for differing values of mixture components.\nrgfm: The RandomFiniteGaussianMixture to which the variational posterior was fit.\n\n\n\n\n\n","category":"type"},{"location":"methods/RandomFiniteGaussianMixture/#BayesDensityCore.varinf-Tuple{RandomFiniteGaussianMixture}","page":"RandomFiniteGaussianMixture","title":"BayesDensityCore.varinf","text":"varinf(\n    rfgm::RandomFiniteGaussianMixture{T};\n    max_iter::Int = 2000\n    rtol::Real    = 1e-6\n) where {T} -> PitmanYorMixtureVIPosterior{T}\n\nFind a variational approximation to the posterior distribution of a RandomFiniteGaussianMixture using mean-field variational inference.\n\nArguments\n\nrfgm: The RandomFiniteGaussianMixture whose posterior we want to approximate.\n\nKeyword arguments\n\nmax_iter: Maximal number of VI iterations. Defaults to 2000.\nrtol: Relative tolerance used to determine convergence. Defaults to 1e-6.\n\nReturns\n\nvip: A RandomFiniteGaussianMixtureVIPosterior object representing the variational posterior.\ninfo: A VariationalOptimizationResult describing the result of the optimization.\n\nnote: Note\nTo perform the optimization for a fixed number of iterations irrespective of the convergence criterion, one can set rtol = 0.0, and max_iter equal to the desired total iteration count. Note that setting rtol to a strictly negative value will issue a warning.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> rfgm = RandomFiniteGaussianMixture(x);\n\njulia> vip = varinf(rfgm);\n\njulia> vip = varinf(rfgm; rtol=1e-7, max_iter=3000);\n\n\n\n\n\n","category":"method"},{"location":"methods/RandomFiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.posterior_components","page":"RandomFiniteGaussianMixture","title":"BayesDensityFiniteGaussianMixture.posterior_components","text":"posterior_components(vip::RandomFiniteGaussianMixtureVIPosterior{T}) where {T} -> DiscreteNonParametric{Int, T}\n\nGet the variational posterior probability mass function of the number of mixture components as a DiscreteNonParametric instance.\n\n\n\n\n\n","category":"function"},{"location":"methods/RandomFiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.maximum_a_posteriori","page":"RandomFiniteGaussianMixture","title":"BayesDensityFiniteGaussianMixture.maximum_a_posteriori","text":"maximum_a_posteriori(\n    vip::RandomFiniteGaussianMixtureVIPosterior{T}\n) where {T} -> FiniteGaussianMixtureVIPosterior{T}\n\nGet the variational posterior distribution that maximizes the approximate posterior probability on the number of components q(K).\n\n\n\n\n\n","category":"function"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"Contributions to BayesDensity.jl are very much welcome! If you have a feature request or would like to contribute, feel free to open an issue or a PR!\n\nContributions of the following kind are especially welcome:\n\nImplementing new Bayesian density estimators. To get started on implementing estimators compatible with the current API, see the \"Implementing new Bayesian density estimators\" tutorial page.\nImplementing new algorithms for existing Bayesian density estimators.\nImproving the performance of the currently implemented algorithms.\nImproving the documentation.","category":"section"},{"location":"api/plotting_api/#Plotting-API","page":"Plotting API","title":"Plotting API","text":"Graphical displays are a powerful tool for providing informative vizual summaries of the result of a given Bayesian inference procedure for a univariate density. BayesDensity makes it easy to plot posterior summaries for f using the results from Markov chain Monte Carlo sampling or variational inference through its extensions for the Makie.jl and Plots.jl packages.\n\nIn addition to documenting the plotting-related public API, this page also showcases the plotting capabilities of the BayesDensityCore package through examples. Although we will not delve deep into implementational details here, some familiarity with Makie.jl or Plots.jl is an advantage when reading this part of the documentation.\n\nThe following sections are structured so that the Makie- and the Plots-portions of the tutorial can be read independently of one another. As such, there is no need for a Makie power-user to read the Plots sections of this page.","category":"section"},{"location":"api/plotting_api/#Plotting-with-Makie.jl","page":"Plotting API","title":"Plotting with Makie.jl","text":"In general, the available plot method for PosteriorSamples and AbstractVIPosterior objects has the following signature:\n\nplot(\n    ps::Union{PosteriorSamples, AbstractVIPosterior},\n    [func = pdf],\n    [t::AbstractVector{<:Real}];\n    ci::Bool = true,\n    level::Real = 0.95,\n    estimate = mean,\n    alpha::Real = 0.25,\n    kwargs...\n)\n\nThe first argument to plot is the posterior distribution, fitted via either Markov chain Monte Carlo or variational inference. The second (optional) positional argument indicates whether to plot estimates of the pdf or the cdf. By default, the estimated pdf is shown. The third (optional) positional argument is the grid at which the pdf or cdf is evaluated to draw the grid. The ci keyword is a boolean, controlling whether or not a credible interval should be drawn (enabled by default). To control the level of the drawn credible interval, set the level keyword argument to the desired confidence level.\n\nOther keyword arguments mostly control the appearance of the drawn lines and credible bands. Of particular note are strokecolor, which controls the color of the density estimate, and color, which controls the color of the credible bands.  The alpha keyword argument controls the transparency of the credible bands. The line width of the density estimate is controlled through the :strokewidth keyword argument.","category":"section"},{"location":"api/plotting_api/#Example","page":"Plotting API","title":"Example","text":"To show the plotting-capabilities of the Makie extension in practice, we start by importing the required packages and fit a BayesDensity model to some simulated data:\n\nusing BayesDensityHistSmoother, CairoMakie, Distributions, Random\nrng = Random.Xoshiro(1)\n\n# Simulate some data from the \"Claw\" density\nd_true = MixtureModel(\n    vcat(Normal(0, 1), [Normal(0.5*j, 0.1) for j in -2:2]),\n    [0.5, 0.1, 0.1, 0.1, 0.1, 0.1]\n)\nx = rand(rng, d_true, 1000)\n\n# Fit the model via MCMC and VI\nhistsmoother = HistSmoother(x)\nposterior_sample = sample(rng, histsmoother, 1100)\nvi_posterior, info = varinf(histsmoother)\n\nHaving fitted the model, we can use the extended plot function to generate various plots from the fitted model objects, be it the variational posterior or the MCMC samples. The most basic usage of the is to simply call plot(posterior_sample) or plot(vi_posterior), which both generate a plot of the estimated posterior mean of f, along with 95 % pointwise credible bands. The code snippet below illustrates how one can customize the posterior plots:\n\nt = LinRange(-3.5, 3.5, 4001)\n\n# Create figure, axes\nfig = Figure(size=(550, 550))\nax1 = Axis(fig[1,1], xlabel=\"x\", ylabel=\"Density\")\nax2 = Axis(fig[1,2], xlabel=\"x\", ylabel=\"Density\")\nax3 = Axis(fig[2,1], xlabel=\"x\", ylabel=\"Cumulative density\")\nax4 = Axis(fig[2,2], xlabel=\"x\", ylabel=\"Cumulative density\")\n\n# Plot estimated density and CI from MCMC samples\nplot!(ax1, posterior_sample, color=:red, strokecolor=:red, label=\"Estimate (CI)\", alpha=0.1)\nylims!(ax1, 0.0, 0.85)\n\n# Compare the posterior median of the VI fit to the true density (without CI)\nplot!(ax2, vi_posterior, pdf, t; ci=false,\n      estimate=median, label=\"Estimate\") # NB! Supplying pdf is redundant here\nlines!(ax2, t, pdf(d_true, t), color=:black, label=\"True pdf\",\n       linestyle=:dash)\nxlims!(ax2, -2.5, 2.5)\nylims!(ax2, 0.0, 0.85)\n\n# Plot the estimated cdf and the CI\nplot!(ax3, posterior_sample, cdf, level=0.99, color=:red, strokecolor=:red, label=\"Estimate (CI)\")\n\n# Compare estimated cdf of the VI fit to the true cdf (without CI)\nplot!(ax4, vi_posterior, cdf, ci=false, label=\"Estimate\")\nlines!(ax4, t, cdf(d_true, t),\n      color = :black, label=\"True cdf\", linestyle=:dash)\nxlims!(ax4, -2.2, 2.2)\n\nfor ax in (ax1, ax2, ax3, ax4)\n    axislegend(ax; position=:lt, framevisible=false, labelsize=10)\nend\n\nfig\n\n(Image: Makie showcase)\n\nMakie.jl plots can also be used to perform model diagnostics for variational inference by plotting the evolution of the evidence lower bound (ELBO) on a per-iteration basis. This can be acheived by calling plot(info) on a VariationalOptimizationResult. Note that this is effectively just a thin wrapper around lines(elbo(info)).","category":"section"},{"location":"api/plotting_api/#Plotting-with-Plots.jl","page":"Plotting API","title":"Plotting with Plots.jl","text":"In general, the available plot method for PosteriorSamples and AbstractVIPosterior objects has the following signature:\n\nplot(\n    ps::Union{PosteriorSamples, AbstractVIPosterior},\n    [func = pdf],\n    [t::AbstractVector{<:Real}];\n    ci::Bool = true,\n    level::Real = 0.95,\n    estimate = mean,\n    alpha::Real = 0.25,\n    kwargs...\n)\n\nThe first argument to plot is the posterior distribution, fitted via either Markov chain Monte Carlo or variational inference. The second (optional) positional argument indicates whether to plot estimates of the pdf or the cdf. By default, the estimated pdf is shown. The third (optional) positional argument is the grid at which the pdf or cdf is evaluated to draw the grid. The ci keyword is a boolean, controlling whether or not a credible interval should be drawn (enabled by default). To control the level of the drawn credible interval, set the level keyword argument to the desired confidence level.\n\nOther keyword arguments mostly control the appearance of the drawn lines and credible bands. Of particular note are color, which controls the color of the density estimate, and fillcolor, which controls the color of the credible bands.  The fillalpha keyword argument controls the transparency of the credible bands.","category":"section"},{"location":"api/plotting_api/#Example-2","page":"Plotting API","title":"Example","text":"To show the plotting-capabilities of the Makie extension in practice, we start by importing the required packages and fit a BayesDensity model to some simulated data:\n\nusing BayesDensityHistSmoother, Plots, Distributions, Random\nrng = Random.Xoshiro(1)\n\n# Simulate some data from the \"Claw\" density\nd_true = MixtureModel(\n    vcat(Normal(0, 1), [Normal(0.5*j, 0.1) for j in -2:2]),\n    [0.5, 0.1, 0.1, 0.1, 0.1, 0.1]\n)\nx = rand(rng, d_true, 1000)\n\n# Fit the model via MCMC and VI\nhistsmoother = HistSmoother(x)\nposterior_sample = sample(rng, histsmoother, 1100)\nvi_posterior, info = varinf(histsmoother)\n\nHaving fitted the model, we can use the extended plot function to generate various plots from the fitted model objects, be it the variational posterior or the MCMC samples. The most basic usage of the is to simply call plot(posterior_sample) or plot(vi_posterior), which both generate a plot of the estimated posterior mean of f, along with 95 % pointwise credible bands. The code snippet below illustrates how one can customize the posterior plots:\n\nt = LinRange(-3.5, 3.5, 4001)\n\n# Create subplots\np1 = plot(xlabel=\"x\", ylabel=\"Density\")\np2 = plot(xlabel=\"x\", ylabel=\"Density\")\np3 = plot(xlabel=\"x\", ylabel=\"Cumulative density\")\np4 = plot(xlabel=\"x\", ylabel=\"Cumulative density\")\n\n# Plot estimated density and CI from MCMC samples\nplot!(p1, posterior_sample, color=:red, fillcolor=:red,\n      label=\"Estimate (CI)\", fillalpha=0.1)\nylims!(p1, 0.0, 0.7)\n\n# Compare the posterior median of the VI fit to the true density (without CI)\nplot!(p2, vi_posterior, pdf, t; ci=false,\n      estimate=median, label=\"Estimate\") # NB! Supplying pdf is redundant here\nplot!(p2, t, pdf(d_true, t), color=:black, label=\"True pdf\",\n       linestyle=:dash)\nxlims!(p2, -2.5, 2.5)\nylims!(p2, 0.0, 0.7)\n\n# Plot the estimated cdf and the CI\nplot!(p3, posterior_sample, cdf, level=0.99, color=:red, fillcolor=:red, label=\"Estimate (CI)\")\n\n# Compare estimated cdf of the VI fit to the true cdf (without CI)\nplot!(p4, vi_posterior, cdf, ci=false, label=\"Estimate\")\nplot!(p4, t, cdf(d_true, t),\n      color = :black, label=\"True cdf\", linestyle=:dash)\nxlims!(p4, -2.2, 2.2)\n\nplot(p1, p2, p3, p4, layout=(2,2), size=(550, 550))\n\n(Image: Plots showcase)\n\nPlots.jl plots can also be used to perform model diagnostics for variational inference by plotting the evolution of the evidence lower bound (ELBO) on a per-iteration basis. This can be acheived by calling plot(info) on a VariationalOptimizationResult. Note that this is effectively just a thin wrapper around plot(elbo(info)).","category":"section"},{"location":"methods/BSplineMixture/#BSplineMixture","page":"BSplineMixture","title":"BSplineMixture","text":"Documentation for B-Spline Mixture Models.\n\nThis model is available through the BayesDensityBSplineMixture package.","category":"section"},{"location":"methods/BSplineMixture/#Module-API","page":"BSplineMixture","title":"Module API","text":"","category":"section"},{"location":"methods/BSplineMixture/#Evaluating-the-pdf-and-cdf","page":"BSplineMixture","title":"Evaluating the pdf and cdf","text":"","category":"section"},{"location":"methods/BSplineMixture/#Utility-functions","page":"BSplineMixture","title":"Utility functions","text":"","category":"section"},{"location":"methods/BSplineMixture/#Markov-chain-Monte-Carlo","page":"BSplineMixture","title":"Markov chain Monte Carlo","text":"","category":"section"},{"location":"methods/BSplineMixture/#Variational-inference","page":"BSplineMixture","title":"Variational inference","text":"","category":"section"},{"location":"methods/BSplineMixture/#BayesDensityBSplineMixture.BSplineMixture","page":"BSplineMixture","title":"BayesDensityBSplineMixture.BSplineMixture","text":"BSplineMixture{T<:Real} <: AbstractBayesDensityModel{T}\n\nStruct representing a B-spline mixture model.\n\nConstructors\n\nBSplineMixture(x::AbstractVector{<:Real}; kwargs...)\nBSplineMixture{T}(x::AbstractVector{<:Real}; kwargs...)\n\nArguments\n\nx: The data vector.\n\nKeyword arguments\n\nK: B-spline basis dimension of a regular augmented spline basis. Defaults to max(100, min(200, ⌈n/5⌉))\nbounds: A tuple giving the support of the B-spline mixture model.\nn_bins: Lower bound on the number of bins used when fitting the BSplineMixture to data. Binned fitting can be disabled by setting this equal to nothing. The default setting uses unbinned fitting if length(x) ≤ 1200 and 1000 bins otherwise.\nprior_global_shape: Shape hyperparameter for the global smoothing parameter τ². Defaults to 1.0.\nprior_global_rate: Rate hyperparameter for the global smoothing parameter τ². Defaults to 1e-3.\nprior_local_shape: Shape hyperparameter for the local smoothing parameters δₖ². Defaults to 0.5.\nprior_local_rate: Rate hyperparameter for the local smoothing parameters δₖ². Defaults to 0.5.\nprior_stdev: Prior standard deviation of the first two unconstrained spline parameters β₁ and β₂. Defaults to 1e5.\n\nReturns\n\nbsm: A B-Spline mixture model object.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> model = BSplineMixture(x)\n200-dimensional BSplineMixture{Float64}:\nUsing 5000 binned observations on a regular grid consisting of 1187 bins.\n support: (-0.05, 1.05)\nHyperparameters:\n prior_global_shape = 1.0, prior_global_rate = 0.001\n prior_local_shape = 0.5, prior_local_rate = 0.5\n\njulia> model = BSplineMixture(x; K = 150, bounds=(0, 1), n_bins=nothing, prior_global_rate = 5e-3);\n\nExtended help\n\nBinned fitting\n\nTo disable binned fitting, one can set n_bins=nothing. Note that the binning is only used as part of the model fitting procedure, and the structure of the resulting fitted model object is the same regardless of whether the binning step is performed or not. Empirically, the results obtained from running the binned and unbinned model fitting procedures tend to be very similar. We therefore recommend using the binned fitting procedure, due to the large improvements in model fitting speed, particularly for larger samples.\n\nFor computational reasons, the supplied number of bins is rounded up to the nearest integer such that the bin boundaries overlap with the knots of the spline basis. This is done to ensure that at most 4 cubic splines have positive integrals over each bin.\n\nHyperparameter selection\n\nThe global variance parameter τ2 and the local variance parameters δ2[k] govern the smoothness of the B-spline mixture prior through the centered random walk prior on β | τ2, δ2:\n\nβ[k+2] = μ[k+2] + 2 {β[k+1] - μ[k+1]} - {β[k] - μ[k]} + τ * δ[k] * ϵ[k],\n\nwhere ϵ[k] is standard normal. The first two parameters β[1] and β[2] are assigned diffuse N(0, σ²) priors.\n\nThe prior distributions of the local and global smoothing parameters are given by\n\nτ² ∼ InverseGamma(a_τ, b_τ)\nδₖ² ∼ InverseGamma(a_δ, b_δ),   1 ≤ k ≤ K-3.\n\nAs noninformative defaults, we suggest using prior_global_shape = 1, prior_global_rate = 1e-3, prior_local_shape = 0.5, prior_local_rate = 0.5 and prior_stdev = 1e5. To control the smoothness in the resulting density estimates, we recommend adjusting the value of prior_global_rate while keeping the other hyperparameters fixed. Setting prior_global_rate to a smaller value generally yields smoother curves. Similar priors for regression models suggest that values in the range [5e-5, 5e-3] are reasonable.\n\n\n\n\n\n","category":"type"},{"location":"methods/BSplineMixture/#Distributions.pdf-Union{Tuple{Vals}, Tuple{Names}, Tuple{BSplineMixture, NamedTuple{Names, Vals}, Real}} where {Names, Vals<:Tuple}","page":"BSplineMixture","title":"Distributions.pdf","text":"pdf(\n    bsm::BSplineMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\npdf(\n    bsm::BSplineMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate f(t  boldsymboleta) for a given BSplineMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain a field named :spline_coefs or :β.\n\n\n\n\n\n","category":"method"},{"location":"methods/BSplineMixture/#Distributions.cdf-Union{Tuple{Vals}, Tuple{Names}, Tuple{BSplineMixture, NamedTuple{Names, Vals}, Real}} where {Names, Vals<:Tuple}","page":"BSplineMixture","title":"Distributions.cdf","text":"cdf(\n    bsm::BSplineMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\ncdf(\n    bsm::BSplineMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate F(t  boldsymboleta) for a given BSplineMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain a field named :spline_coefs or :β.\n\n\n\n\n\n","category":"method"},{"location":"methods/BSplineMixture/#BayesDensityCore.hyperparams-Tuple{BSplineMixture}","page":"BSplineMixture","title":"BayesDensityCore.hyperparams","text":"hyperparams(\n    bsm::BSplineMixture{T}\n) where {T} -> @NamedTuple{prior_global_shape::T, prior_global_rate::T, prior_local_shape::T, prior_local_rate::T, prior_stdev::T}\n\nReturns the hyperparameters of the B-Spline mixture model bsm as a NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"methods/BSplineMixture/#StatsBase.sample-Tuple{AbstractRNG, BSplineMixture, Int64}","page":"BSplineMixture","title":"StatsBase.sample","text":"sample(\n    [rng::Random.AbstractRNG],\n    bsm::BSplineMixture{T},\n    n_samples::Int;\n    n_burnin::Int              = min(1000, div(n_samples, 5)),\n    initial_params::NamedTuple = get_default_initparams_mcmc(bsm)\n) where {T} -> PosteriorSamples{T}\n\nGenerate n_samples posterior samples from a BSplineMixture using an augmented Gibbs sampler.\n\nArguments\n\nrng: Optional random seed used for random variate generation.\nbsm: The BSplineMixture object for which posterior samples are generated.\nn_samples: The total number of samples (including burn-in).\n\nKeyword arguments\n\nn_burnin: Number of burn-in samples.\ninitial_params: Initial values used in the MCMC algorithm. Should be supplied as a NamedTuple with fields :β and :τ2, where :β is a K-1-dimensional vector and τ2 is a positive scalar.\n\nReturns\n\nps: A PosteriorSamples object holding the posterior samples and the original model object.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> bsm = BSplineMixture(x);\n\njulia> ps = sample(Xoshiro(1), bsm, 5000);\n\n\n\n\n\n","category":"method"},{"location":"methods/BSplineMixture/#BayesDensityBSplineMixture.BSplineMixtureVIPosterior","page":"BSplineMixture","title":"BayesDensityBSplineMixture.BSplineMixtureVIPosterior","text":"BSplineMixtureVIPosterior{T<:Real, A<:MvNormalCanon{T}, B<:InverseGamma{T}, M<:BSplineMixture} <: AbstractVIPosterior{T}\n\nStruct representing the variational posterior distribution of a BSplineMixture.\n\nFields\n\nq_β: Distribution representing the optimal variational density q*(β).\nq_τ: Distribution representing the optimal variational density q*(τ²).\nq_δ: Product distribution corresponding to the optimal variational density q*(δ²).\nbsm: The BSplineMixture to which the variational posterior was fit.\n\n\n\n\n\n","category":"type"},{"location":"methods/BSplineMixture/#BayesDensityCore.varinf-Tuple{BSplineMixture}","page":"BSplineMixture","title":"BayesDensityCore.varinf","text":"varinf(\n    bsm::BSplineMixture{T};\n    initial_params::NamedTuple = _get_default_initparams(bsm),\n    max_iter::Int              = 2000\n    rtol::Real                 = 1e-6\n) where {T} -> BSplineMixtureVIPosterior{T}\n\nFind a variational approximation to the posterior distribution of a BSplineMixture using mean-field variational inference.\n\nArguments\n\nbsm: The BSplineMixture whose posterior we want to approximate.\n\nKeyword arguments\n\ninitial_params: Initial values of the VI parameters μ_opt inv_Σ_opt, b_τ_opt and b_δ_opt, supplied as a NamedTuple.\nmax_iter: Maximal number of VI iterations. Defaults to 2000.\nrtol: Relative tolerance used to determine convergence. Defaults to 1e-6.\n\nReturns\n\nvip: A BSplineMixtureVIPosterior object representing the variational posterior.\ninfo: A VariationalOptimizationResult describing the result of the optimization.\n\nnote: Note\nTo sample for a fixed number of iterations irrespective of the convergence criterion, one can set rtol = 0.0, and max_iter equal to the desired total iteration count. Note that setting rtol to a strictly negative value will issue a warning.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> bsm = BSplineMixture(x);\n\njulia> vip, info = varinf(bsm);\n\njulia> vip, info = varinf(bsm; rtol=1e-7, max_iter=3000);\n\nExtended help\n\nConvergence\n\nThe criterion used to determine convergence is that the relative change in the ELBO falls below the given rtol.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/add_new_models/#Implementing-new-Bayesian-density-estimators","page":"Implementing new Bayesian density estimators","title":"Implementing new Bayesian density estimators","text":"The following page provides a tutorial on how to implement new Bayesian density estimators compatible with the BayesDensity.jl-package. Prior to reading this tutorial, one should have already familiarized oneself with the general package API, for instance by reading the General API documentation.\n\nIn order to be able to follow this tutorial, it is advantageous to have some prior exposure to data-augmentation schemes, Gibbs sampling and mean-field variational inference. A good introduction to all three topics can be found in Bishop (2006).\n\nnote: Note\nThe focus of the following tutorial is to present how one can implement new Bayesian models in a BayesDensity-compatible way. As a result, the implementation presented here is by no means optimal in terms of computational efficiency or numerical stability for this particular example.","category":"section"},{"location":"tutorials/add_new_models/#Bayesian-inference-for-Bernstein-densities.","page":"Implementing new Bayesian density estimators","title":"Bayesian inference for Bernstein densities.","text":"For our tutorial, we will illustrate by focusing on a Bayesian Bernstein-type density estimator for data supported on the unit interval.[1] This section provides the theoretical background for the model we will later implement as an example, and can be skipped by readers who are more interested in the details of the implementation itself.\n\n[1]: A Bayesian Bernstein-type estimator, where the number K of mixture components is treated as a further random variable has been proposed by Petrone (1999).\n\nGiven a positive integer K, we say that f is a Bernstein density if we can write\n\nf(x) = sum_k=1^K theta_k varphi_k(x) quad xin 0 1\n\nwhere varphi_k(cdot) is the density of the mathrmBeta(k K-k+1)-distribution and boldsymboltheta in boldsymbolvarthetain 01^K colon sum_k vartheta_k = 1 . The corresponding cumulative distribution function F is then\n\nF(x) = sum_k=1^K theta_k int_0^xvarphi_k(t) textdt quad xin 0 1\n\nowing to the linearity of the integral.\n\nOur main motivation for considering the Bernstein model is that under mild regularity conditions on the true density f_0, the Bernstein density f can approximate f_0 to an arbitrary degree of precision with respect to a suitable metric, such as the total variation distance, provided K is sufficiently large.\n\nFor a Bayesian treatment of the Bernstein density model, we impose a mathrmDirichlet(boldsymbola)-prior distribution on boldsymboltheta, where boldsymbola = (aa ldots a) for some a0. Given an observed independent and identically distributed sample boldsymbolx = (x_1 x_2 ldots x_n), the likelihood of the observed sample under the Bernstein model for f is\n\np(boldsymbolx boldsymboltheta) = prod_i=1^n sum_k=1^K theta_k varphi_k(x_i)\n\nThe form taken by the likelihood function above makes Bayesian inference challenging due to the fact that the resulting posterior distribution is analytically intractable. However, by augmenting the data with latent variables boldsymbolz in 12ldots K^n, it is possible to perform posterior inference very efficiently through Gibbs sampling or mean-field VI. Another possible way of defining the Bernstein density model is to let x_i   boldsymboltheta z_i = k sim varphi_k and p(z_i = k) = theta_k for all i, as this leads to the same likelihood function as previously when the z_i are marginalized out.`\n\nUnder this data augmentation strategy it can then be shown that joint posterior of boldsymboltheta boldsymbolz is\n\np(boldsymboltheta boldsymbolz  boldsymbolx) propto prod_k = 1^K theta_k^N_k + a - 1 prod_i=1^n prod_k=1^K varphi_k(x_i)^mathbf1_k(z_i)\n\nwhere mathbf1_k(cdot) is the indicator function and N_k = sum_i=1^n mathbf1_k(z_i).","category":"section"},{"location":"tutorials/add_new_models/#Gibbs-sampling","page":"Implementing new Bayesian density estimators","title":"Gibbs sampling","text":"To write down a Gibbs sampler for this model, we need to derive the full conditional distributions of boldsymboltheta and boldsymbolz. In this case, direct inspection of the joint posterior shows that\n\np(boldsymboltheta  boldsymbolz boldsymbolx) = mathrmDirichlet(boldsymbola + boldsymbolN)\n\nwhere boldsymbolN = (N_1 N_2 ldots N_K). The full conditional distributions for boldsymbolz are \n\np(boldsymbolz  boldsymboltheta boldsymbolx) propto prod_i=1^n prod_k=1^K bigtheta_kvarphi_k(x_i)big^mathbf1_k(z_i)\n\nHence, we see that z_1 ldots z_n are independent given boldsymboltheta boldsymbolx, with p(z_i = k  boldsymboltheta boldsymbolx) propto theta_k varphi_k(x_i).","category":"section"},{"location":"tutorials/add_new_models/#Variational-inference","page":"Implementing new Bayesian density estimators","title":"Variational inference","text":"For the Bernstein density , it is relatively straightforward to implement a mean-field variational inference scheme. Here, we approximate the joint posterior p(boldsymboltheta boldsymbolz boldsymbolx) via a distribution q which satisfies the following independence assumption:\n\nq(boldsymboltheta boldsymbolz) = q(boldsymboltheta) q(boldsymbolz)\n\nIt can be shown [see e.g. Ormerod and Wand (2010)] that the optimal q densities are given by\n\nbeginaligned\n  q(boldsymboltheta) propto expbigmathbbE_boldsymbolzbiglog p(boldsymboltheta boldsymbolz)bigbig\n  q(boldsymbolz) propto expbigmathbbE_boldsymbolthetabiglog p(boldsymboltheta boldsymbolz)bigbig\nendaligned\n\nwhere the expectations are taken with respect to q(boldsymbolz) and q(boldsymboltheta), respectively. This result leads to the iterative coordinate-wise ascent variational inference algorithm (CAVI) for finding the optimal q densities, where we cyclically update q(boldsymboltheta) and q(boldsymbolz) until some convergence criterion has been met. An oft-used convergence criterion for this purpose is the evidence lower bound, (ELBO):\n\nmathrmELBO(q) = exp BigmathbbE_boldsymboltheta boldsymbolzBig(log fracp(boldsymbolx boldsymboltheta boldsymbolz)q(boldsymboltheta boldsymbolz)Big)Big\n\nFor our particular example it can be shown that the optimal q densities are:\n\nq(boldsymboltheta) is the mathrmDirichlet(boldsymbola + boldsymbolr)-density, where r_k = sum_i=1^n q(z_i = k).\nq(boldsymbolz) = prod_i=1^n q(z_i), where q(z_i) is the probability mass function of a categorical distribution on 12ldots K with q(z_i = k) propto varphi_k(x_i) expbigpsi(a_k + r_k)big, where psi(cdot) denotes the digamma function.\n\nAn expression for the ELBO of this model is as follows:\n\nbeginaligned\n  mathrmELBO(q) = sum_i=1^n sum_k=1^K q(z_i = k) biglog b_k(x_i) - log q(z_i = k)big  + sum_k=1^K biglog Gamma(a + r_k) - log Gamma(a)big  - log Gamma(aK+n) + log Gamma(aK)\nendaligned","category":"section"},{"location":"tutorials/add_new_models/#Implementation","page":"Implementing new Bayesian density estimators","title":"Implementation","text":"We start by importing the required packages:\n\nusing BayesDensityCore, Distributions, Random, StatsBase","category":"section"},{"location":"tutorials/add_new_models/#Model-struct-and-pdf","page":"Implementing new Bayesian density estimators","title":"Model struct and pdf","text":"The first step to implementing the Bernstein density model in a BayesDensity-compatible way is to define a model struct which is a subtype of AbstractBayesDensityModel:\n\nstruct BernsteinDensity{T<:Real, D<:NamedTuple} <: AbstractBayesDensityModel{T}\n    data::D # NamedTuple holding data\n    K::Int  # Basis dimension\n    a::T    # Symmetric Dirichlet parameter.\n    function BernsteinDensity{T}(x::AbstractVector{<:Real}, K::Int; a::Real=1.0) where {T<:Real}\n        φ_x = Matrix{T}(undef, (length(x), K))\n        for i in eachindex(x)\n            for k in 1:K\n                φ_x[i, k] = pdf(Beta(k, K - k + 1), x[i])\n            end\n        end\n        data = (x = x, n = length(x), φ_x = φ_x)\n        return new{T, typeof(data)}(data, K, T(a))\n    end\nend\nBernsteinDensity(args...; kwargs...) = BernsteinDensity{Float64}(args...; kwargs...) # For convenience\n\nIn the above implementation, we store the values of varphi_k(x_i) for 1 leq i leq n and 1 leq k leq K, as these values are reused repeatedly in the model fitting processes later. We also mantain a copy of the original dataset x in the data field. By default, the original data stored under the field bdm.data.x is used to select a default grid for the first coordinate axis when plotting fitted model objects via the fallback implementation of default_grid_points. If the original data cannot be found under bdm.data.x, then trying to plot a PosteriorSamples or AbstractVIPosterior object without supplying a plotting grid as the second argument will fail, in which case implementing default_plot_grid for this model class will resolve this issue.\n\nReturning to our specific example, we note that the Bernstein model is always supported on 0 1, so it may make more sense to use a default grid spanning this entire interval. A possible implementation of default_grid_points for this purpose is as follows:\n\nBayesDensityCore.default_grid_points(::BernsteinDensity{T}) where {T} = LinRange{T}(0, 1, 2001)\n\nIn order to be able use all the functionality of BayesDensityCore, we also need to implement an equality method for our new type. In this case, this is just a matter of checking that all the fields of two such objects are equal:\n\nBase.:(==)(bd1::BernsteinDensity, bd2::BernsteinDensity) = bd1.data == bd2.data && bd1.K == bd2.K && bd1.a == bd2.a\n\nNext, we implement a method that calculates the pdf of the model when the parameters of the model are given. The pdf method should always receive the model object as the first argument, the parameters as the second argument and the point(s) at which the density should be evaluated as the third. In the implementation presented below, we take in a NamedTuple with a single field named θ which represents the mixture probabilities.\n\nfunction Distributions.pdf(bdm::BernsteinDensity{T, D}, params::NamedTuple, t::S) where {T<:Real, D, S<:Real}\n    K = bdm.K\n    (; θ) = params\n    f = zero(promote_type(T, S))\n    for k in 1:K\n        f += θ[k] * pdf(Beta(k, K - k + 1), t)\n    end\n    return f\nend\n\nThe BayesDensityCore module provides generic fallback methods for the cases where params is given as a Vector of NamedTuples and when t is a vector. However, as noted in the general API, it is recommended that most models provide specialized methods for vectors of parameters and vectors of evaluation points, as it is often possible to implement batch evaluation more efficiently, e.g. by leveraging BLAS calls instead of loops, when the parameters and the evaluation grid are provided in batches.\n\nNext, we need to implement the cdf method. Owing to the nice structure of the cdf F in this example, this is no more complicated than implementing the pdf:\n\nfunction Distributions.cdf(bdm::BernsteinDensity{T, D}, params::NamedTuple, t::S) where {T<:Real, D, S<:Real}\n    K = bdm.K\n    (; θ) = params\n    f = zero(promote_type(T, S))\n    for k in 1:K\n        f += θ[k] * cdf(Beta(k, K - k + 1), t)\n    end\n    return f\nend\n\nIn general, it is good practice to also implement the support and hyperparams methods for new models. Note that for the Bernstein density model, the support is always equal to the unit interval, and the only hyperparameter is the scalar value a (here, we treat K as fixed). Hence, the following provides appropriate implementations of the aforementioned methods:\n\nBayesDensityCore.support(::BernsteinDensity{T, D}) where {T, D} = (T(0.0), T(1.0))\nBayesDensityCore.hyperparams(bdm::BernsteinDensity) = (a = bdm.a,)","category":"section"},{"location":"tutorials/add_new_models/#Gibbs-sampler","page":"Implementing new Bayesian density estimators","title":"Gibbs sampler","text":"We now turn our attention to implementing the Gibbs sampler itself. All BayesDensity-compatible Markov chain Monte Carlo samplers should overload the sample method. This function should always take in a random seed as the first argument, the density model object as the second argument and the total number of samples (including burn-in) as the third argument. In addition, the number of burn-in samples must be provided as a keyword argument.\n\nThe sample method should always return an object of type PosteriorSamples in order to be compatible with the rest of the package. The samples generated during the MCMC routine should be stored in a subtype of AbstractVector, where the type of the elements are compatible with the function signature for the implemented pdf method. Since our implementation of the pdf method takes in a NamedTuple as the parameters argument, we store the generated samples in a vector of NamedTuples in the implementation shown below:\n\nfunction StatsBase.sample(rng::AbstractRNG, bdm::BernsteinDensity{T, D}, n_samples::Int; n_burnin=min(div(length(x), 5), 1000), init_params::NamedTuple=(θ = fill(1/K, K),)) where {T, D}\n    (; K, data, a) = bdm\n    (; x, n, φ_x) = data\n\n    a_vec = fill(a, K) # Dirichlet prior parameter\n\n    θ = T.(init_params.θ) # Initialize θ as the uniform vector\n    probs = Vector{T}(undef, K) # Vector used to store intermediate calculations of p(zᵢ|θ, x)\n\n    # Store samples as a vector of NamedTuples\n    samples = Vector{NamedTuple{(:θ,), Tuple{Vector{Float64}}}}(undef, n_samples)\n\n    for m in 1:n_samples\n        N = zeros(Int, K) # N[k] = number of z[i] equal to k.\n        for i in 1:n\n            for k in 1:K\n                probs[k] = θ[k] * φ_x[i, k]\n            end\n            probs = probs / sum(probs)\n            N .+= rand(rng, Multinomial(1, probs)) # sample zᵢ ∼ p(zᵢ|θ, x)\n        end\n        θ = rand(rng, Dirichlet(a_vec + N)) # sample θ ∼ p(θ|z, x)\n        samples[m] = (θ = θ,) # store the current value of θ\n    end\n    return PosteriorSamples{T}(samples, bdm, n_samples, n_burnin)\nend\n\nThe above implementation allows the user to supply the initial value of theta used when performing the first iteration of the Gibbs sampler, via a NamedTuple to match the data structure we use to store the samples.\n\nnote: Note\nThe convention adopted by the current set of BayesDensity models is that when during an MCMC run, only model pararameters should be stored, and not auxilliary variables which are only introduced in order to facilitate efficient computation. In this case, we therefore do not store the z_i in the model object returned by this method.","category":"section"},{"location":"tutorials/add_new_models/#Example-usage","page":"Implementing new Bayesian density estimators","title":"Example usage","text":"Having implemented the model struct and the pdf- and sample methods, we can run the MCMC algorithm and perform posterior inference as with any of the other density esitmators implemented in this package:\n\nd_true = Kumaraswamy(2, 5) # Simulate some data from a density supported on [0, 1]\nrng = Xoshiro(1) # for reproducibility\nx = rand(rng, d_true, 1000)\n\nK = 20\nbdm = BernsteinDensity(x, K) # Create Bernstein density model object (a = 1)\nps = sample(rng, bdm, 1_000; n_burnin=500) # Run MCMC\n\nmedian(ps, 0.5) # Compute the posterior median of f(0.5)\n\nFor instance, we can visualize the posterior fit by plotting the posterior means of f(t) and F(t) along with 95 % pointwise credible bands:\n\nusing CairoMakie\nt = LinRange(0, 1, 1001) # Grid for plotting\n\nfig = Figure(size=(600, 320))\nax1 = Axis(fig[1,1], xlabel=\"x\", ylabel=\"Density\")\nplot!(ax1, ps, t, label=\"MCMC\") # Plot the posterior mean and credible bands:\nlines!(ax1, t, pdf(d_true, t), label=\"Truth\", color=:black) # Also plot truth for comparison\n\nax2 = Axis(fig[1,2], xlabel=\"x\", ylabel=\"Cumulative distribution\")\nplot!(ax2, ps, cdf, t, label=\"MCMC\")\nlines!(ax2, t, cdf(d_true, t), label=\"Truth\", color=:black)\n\nLegend(fig[1,3], ax1, framevisible=false)\n\nfig\n\n(Image: Bernstein MCMC fit)","category":"section"},{"location":"tutorials/add_new_models/#Mean-field-variational-inference","page":"Implementing new Bayesian density estimators","title":"Mean-field variational inference","text":"Before getting started on implementing a variational inference algorithm, we first need to define a new struct that represents the variational posterior distribution. To make the resulting variational posterior compatible with the BayesDensityCore interface, the new struct should be a subtype of AbstractVIPosterior.\n\nAlthough not strictly required in order to make the variational posterior struct BayesDensity-compatible, it is customary to have the variational posterior distribution store the variational densities as Distributions-objects, in addition to storing the original model object.\n\nThe implementation below stores the variational density q(boldsymboltheta), along with the Bernstein-density model object. We also create a default constructor which takes in the vector boldsymbolr resulting from the CAVI algorithm, along with the original model object:\n\nstruct BernsteinDensityVIPosterior{T<:Real, D<:Dirichlet{T}, M<:BernsteinDensity} <: AbstractVIPosterior{T}\n    q_θ::D\n    model::M\n    function BernsteinDensityVIPosterior{T}(r::AbstractVector{<:Real}, model::M) where {T<:Real, M<:BernsteinDensity}\n        a = hyperparams(model).a\n        K = model.K\n        q_θ = Dirichlet{T}(fill(a, K) + r)\n        return new{T, Dirichlet{T}, M}(q_θ, model)\n    end\nend\n\nIt is also recommended to implement the model method, so that the user can easily extract the model to which the variational posterior was fitted:\n\nBayesDensityCore.model(vip::BernsteinDensityVIPosterior) = vip.model\n\nNext, we need to implement a method for generating samples from the variational posterior distribution, i.e. sampling from q(boldsymboltheta). This is achieved by implementing the sample method:\n\nfunction StatsBase.sample(rng::AbstractRNG, vip::BernsteinDensityVIPosterior{T,D, M}, n_samples::Int) where {T, D, M}\n    q_θ = vip.q_θ\n    samples = Vector{NamedTuple{(:θ,), Tuple{Vector{Float64}}}}(undef, n_samples)\n    for m in 1:n_samples\n        θ = rand(rng, q_θ)\n        samples[m] = (θ = θ,)\n    end\n    # Note that we return independent samples here, so burn-in is not needed\n    return PosteriorSamples{T}(samples, model(vip), n_samples, 0)\nend\n\nHaving implemented a struct for storing the variational posterior, we can now turn our attention to the optimization procedure itself. To start, we implement the ELBO, which we will need to determine convergence later. Note that in the implementation below, we assume that the values of q(z_i = k) are stored in a n times K matrix omega, so that omega_ik = q(z_i = k).\n\nfunction Bernstein_ELBO(model::BernsteinDensity{T, D}, r::AbstractVector{<:Real}, ω::AbstractMatrix{<:Real}) where {T, D}\n    (; data, K, a) = model\n    (; x, n, φ_x) = data\n    logφ_x = log.(φ_x)\n    ELBO = loggamma(a*K) - loggamma(a*K+n)\n    ELBO += sum(loggamma.(r .+ a)) - K*loggamma(a)\n    for k in 1:K\n        for i in 1:n\n            ELBO += ω[i,k]*(logφ_x[i,k] - log(ω[i,k]))\n        end\n    end\n    return ELBO\nend\n\nFinally, we are ready to implement the optimization procedure itself by overloading varinf. Note that this function should return a 2-tuple, consisting of the fitted variational posterior itself and an object of type VariationalOptimizationResult.\n\nusing SpecialFunctions # For the digamma-function\n\nfunction BayesDensityCore.varinf(model::BernsteinDensity{T, D}; max_iter::Int=1000, rtol::Real=1e-4) where {T, D}\n    (; data, K, a) = model\n    (; x, n, φ_x) = data\n\n    # Initialize the latent variables ω[i,k] = q(z_i = k) to 1/K:\n    ω = fill(T(1/K), (n, K))\n    r = fill(a + n/K, K)\n\n    # CAVI optimization loop\n    ELBO_prev = T(-1)\n    ELBO = Vector{T}(undef, max_iter)\n    converged = false\n    iter = 1\n    while !converged && iter <= max_iter\n        # Update q(θ)\n        r = fill(a, K) + vec(sum(ω, dims=1))\n\n        # Update q(z)\n        for i in 1:n\n            # Compute q(z_i = k) up to proportionality\n            for k in 1:K\n               ω[i,k] = φ_x[i,k] * exp(digamma(a + r[k])) \n            end\n            # Normalize so that the rows of ω sum to 1:\n            ω[i,:] = ω[i,:] / sum(ω[i,:])\n        end\n\n        # Check if the procedure has converged:\n        ELBO[iter] = Bernstein_ELBO(model, r, ω)\n\n        # Run at least two iterations\n        converged = (abs(ELBO_prev - ELBO[iter]) / ELBO_prev <= rtol) && iter > 1\n        ELBO_prev = ELBO[iter]\n        iter += 1\n    end\n\n    # Print a warning if the procedure fails to converge within the maximum number of iterations\n    converged || @warn \"Maximum number of iterations reached.\"\n    \n    posterior = BernsteinDensityVIPosterior{T}(r, model)\n    info = VariationalOptimizationResult{T}(ELBO[1:iter-1], converged, iter-1, rtol, posterior)\n    return posterior, info\nend","category":"section"},{"location":"tutorials/add_new_models/#Example-usage-2","page":"Implementing new Bayesian density estimators","title":"Example usage","text":"Having implemented the varinf method, we can now perform variational inference for the BernsteinDensity model just as easily as for any other BayesDensityCore-compatible model. The example below shows how to fit a variational posterior to the Bernstein model for a simulated dataset:\n\nd_true = Kumaraswamy(2, 5) # Simulate some data from a density supported on [0, 1]\nrng = Xoshiro(1) # for reproducibility\nx = rand(rng, d_true, 1_000)\n\nK = 20\nbdm = BernsteinDensity(x, K) # Create Bernstein density model object (a = 1)\nvip, info = varinf(bdm) # Compute the variational posterior.\n\nmean(vip, 0.2) # Compute the posterior mean of f(0.2)\n\nFor instance, we can visualize the variational posterior fit by displaying the (variational) posterior means of f(t) and F(t) along with 95 % pointwise credible bands:\n\nusing CairoMakie\nt = LinRange(0, 1, 1001) # Grid for plotting\n\nfig = Figure(size=(600, 320))\nax1 = Axis(fig[1,1], xlabel=\"x\", ylabel=\"Density\")\nplot!(ax1, vip, t, label=\"VI\") # Plot the posterior mean and credible bands:\nlines!(ax1, t, pdf(d_true, t), label=\"Truth\", color=:black) # Also plot truth for comparison\n\nax2 = Axis(fig[1,2], xlabel=\"x\", ylabel=\"Cumulative distribution\")\nplot!(ax2, vip, cdf, t, label=\"VI\")\nlines!(ax2, t, cdf(d_true, t), label=\"Truth\", color=:black)\n\nLegend(fig[1,3], ax1, framevisible=false)\n\nfig\n\n(Image: Bernstein variational fit)\n\nWe can also verify that the ELBO has converged:\n\nfig = Figure(size=(400, 350))\nax = Axis(fig[1,1], xlabel=\"Iteration\", ylabel=\"ELBO\")\nlines!(ax, info)\nfig\n\n(Image: Bernstein elbo)","category":"section"},{"location":"methods/FiniteGaussianMixture/#FiniteGaussianMixture","page":"FiniteGaussianMixture","title":"FiniteGaussianMixture","text":"Documentation for finite Gaussian mixture models, with a fixed number of mixture components.\n\nThis model is available through the BayesDensityFiniteGaussianMixture package.\n\nFor Markov chain Monte Carlo based inference, this module implements a Gibbs sampling approach. The algorithm used is essentially the Gibbs sampler sweep (excluding the reversible jump-move) of Richardson and Green (1997). For variational inference, we implement a variant of the algorithm 5 in Ormerod and Wand (2010). Note that our version also includes an additional hyperprior on the rate parameters of the mixture scales and that the algorithm has been adjusted to account for this fact.","category":"section"},{"location":"methods/FiniteGaussianMixture/#Module-API","page":"FiniteGaussianMixture","title":"Module API","text":"","category":"section"},{"location":"methods/FiniteGaussianMixture/#Evaluating-the-pdf-and-cdf","page":"FiniteGaussianMixture","title":"Evaluating the pdf and cdf","text":"","category":"section"},{"location":"methods/FiniteGaussianMixture/#Utility-functions","page":"FiniteGaussianMixture","title":"Utility functions","text":"","category":"section"},{"location":"methods/FiniteGaussianMixture/#Markov-chain-Monte-Carlo","page":"FiniteGaussianMixture","title":"Markov chain Monte Carlo","text":"","category":"section"},{"location":"methods/FiniteGaussianMixture/#Variational-inference","page":"FiniteGaussianMixture","title":"Variational inference","text":"","category":"section"},{"location":"methods/FiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.FiniteGaussianMixture","page":"FiniteGaussianMixture","title":"BayesDensityFiniteGaussianMixture.FiniteGaussianMixture","text":"FiniteGaussianMixture{T<:Real} <: AbstractBayesDensityModel{T}\n\nStruct representing a finite Gaussian mixture model with a fixed number of components.\n\nConstructors\n\nFiniteGaussianMixture(x::AbstractVector{<:Real}, K::Int; kwargs...)\nFiniteGaussianMixture{T}(x::AbstractVector{<:Real}, K::Int; kwargs...)\n\nArguments\n\nx: The data vector.\nK: The number of mixture components.\n\nKeyword arguments\n\nprior_strength: Strength parameter of the symmetric Dirichlet prior on the mixture weights. E.g. the prior is Dirichlet(strength, ..., strength). Defaults to 1.0.\nprior_location: Prior mean of the location parameters μ[k]. Defaults to the midpoint of the minimum and maximum values in the sample.\nprior_variance: The prior variance of the location parameter μ[k]. Defaults to the sample range.\nprior_shape: Prior shape parameter of the squared scale parameters σ2[k]: Defaults to 2.0.\nhyperprior_shape: Prior shape parameter of the hyperprior on the rate parameter of σ2[k]. Defaults to 0.2.\nhyperprior_rate: Prior rate parameter of the hyperprior on the rate parameter of σ2[k]. Defaults to 0.2*R^2, where R is the sample range.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> fgm = FiniteGaussianMixture(x, 5)\nFiniteGaussianMixture{Float64} with 5 components.\nUsing 5000 observations.\nHyperparameters:\n prior_location = 0.5, prior_variance = 1.0\n prior_shape = 2.0, hyperprior_shape = 0.2, hyperprior_rate = 10.0\n prior_strength = 1.0\n\njulia> fgm = FiniteGaussianMixture(x; prior_strength=10);\n\n\n\n\n\n","category":"type"},{"location":"methods/FiniteGaussianMixture/#Distributions.pdf-Tuple{FiniteGaussianMixture, NamedTuple, Real}","page":"FiniteGaussianMixture","title":"Distributions.pdf","text":"pdf(\n    bsm::FiniteGaussianMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\npdf(\n    bsm::FiniteGaussianMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate f(t  boldsymboleta) for a given FiniteGaussianMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain fields named :μ, :σ2, :w and optionally :β.\n\n\n\n\n\n","category":"method"},{"location":"methods/FiniteGaussianMixture/#Distributions.cdf-Tuple{FiniteGaussianMixture, NamedTuple, Real}","page":"FiniteGaussianMixture","title":"Distributions.cdf","text":"cdf(\n    bsm::FiniteGaussianMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\ncdf(\n    bsm::FiniteGaussianMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate F(t  boldsymboleta) for a given FiniteGaussianMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain fields named :μ, :σ2, :w and optionally :β.\n\n\n\n\n\n","category":"method"},{"location":"methods/FiniteGaussianMixture/#BayesDensityCore.hyperparams-Tuple{FiniteGaussianMixture}","page":"FiniteGaussianMixture","title":"BayesDensityCore.hyperparams","text":"hyperparams(\n    gm::FiniteGaussianMixture{T}\n) where {T} -> @NamedTuple{prior_strength::T, prior_location::T, prior_variance::T, prior_shape::T, prior_rate::T}\n\nReturns the hyperparameters of the finite Gaussian mixture model gm as a NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"methods/FiniteGaussianMixture/#StatsBase.sample-Tuple{AbstractRNG, FiniteGaussianMixture, Int64}","page":"FiniteGaussianMixture","title":"StatsBase.sample","text":"sample(\n    [rng::Random.AbstractRNG],\n    fgm::FiniteGaussianMixture{T},\n    n_samples::Int;\n    n_burnin::Int              = min(1000, div(n_samples, 5)),\n    initial_params::NamedTuple = _get_default_initparams_mcmc(hs)\n) where {T} -> PosteriorSamples{T}\n\nGenerate n_samples posterior samples from a FiniteGaussianMixture using an augmented Gibbs sampler.\n\nArguments\n\nrng: Optional random seed used for random variate generation.\nfgm: The FiniteGaussianMixture object for which posterior samples are generated.\nn_samples: The total number of samples (including burn-in).\n\nKeyword arguments\n\nn_burnin: Number of burn-in samples.\ninitial_params: Initial values used in the MCMC algorithm. Should be supplied as a NamedTuple with fields :μ, :σ2 and :w, where all are K-dimensional vectors.\n\nThe following constraints must also be satisfied: σ2[k]>0 for all k and w[k]≥0 for all k and sum(w) ≈ 1\n\nReturns\n\nps: A PosteriorSamples object holding the posterior samples and the original model object.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> fgm = FiniteGaussianMixture(x, 2)\n\njulia> ps1 = sample(fgm, 5_000);\n\njulia> ps2 = sample(fgm, 5_000; n_burnin=2_000, initial_params = (μ = [0.2, 0.8], σ2 = [1.0, 2.0], w = [0.7, 0.3]));\n\n\n\n\n\n","category":"method"},{"location":"methods/FiniteGaussianMixture/#BayesDensityFiniteGaussianMixture.FiniteGaussianMixtureVIPosterior","page":"FiniteGaussianMixture","title":"BayesDensityFiniteGaussianMixture.FiniteGaussianMixtureVIPosterior","text":"FiniteGaussianMixtureVIPosterior{T<:Real} <: AbstractVIPosterior{T}\n\nStruct representing the variational posterior distribution of a FiniteGaussianMixture.\n\nFields\n\nq_w: Distribution representing the optimal variational densities of the component weights q*(w|K).\nq_μ: Product distribution representing the optimal variational densitiy of the component means q*(μ|K).\nq_σ2: Product distribution representing the optimal variational densitiy of the component variances q*(σ2|K).\nq_β: The optimal variational density q*(β|k) of the rate hyperparameter of the component variance σ2[k].\nfgm: The FiniteGaussianMixture to which the variational posterior was fit.\n\n\n\n\n\n","category":"type"},{"location":"methods/FiniteGaussianMixture/#BayesDensityCore.varinf-Tuple{FiniteGaussianMixture}","page":"FiniteGaussianMixture","title":"BayesDensityCore.varinf","text":"varinf(\n    fgm::FiniteGaussianMixture{T};\n    initial_params::NamedTuple = _get_default_initparams(x),\n    max_iter::Int              = 2000\n    rtol::Real                 = 1e-6\n) where {T} -> PitmanYorMixtureVIPosterior{T}\n\nFind a variational approximation to the posterior distribution of a FiniteGaussianMixture using mean-field variational inference.\n\nArguments\n\nfgm: The FiniteGaussianMixture whose posterior we want to approximate.\n\nKeyword arguments\n\ninitial_params: Initial values of the VI parameters dirichlet_params location_params, variance_params, shape_params and rate_params, supplied as a NamedTuple.\nmax_iter: Maximal number of VI iterations. Defaults to 1000.\nrtol: Relative tolerance used to determine convergence. Defaults to 1e-6.\n\nReturns\n\nvip: A FiniteGaussianMixtureVIPosterior object representing the variational posterior.\ninfo: A VariationalOptimizationResult describing the result of the optimization.\n\nnote: Note\nTo perform the optimization for a fixed number of iterations irrespective of the convergence criterion, one can set rtol = 0.0, and max_iter equal to the desired total iteration count. Note that setting rtol to a strictly negative value will issue a warning.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> fgm = FiniteGaussianMixture(x, 10);\n\njulia> vip, info = varinf(fgm);\n\njulia> vip, info = varinf(fgm; rtol=1e-7, max_iter=3000);\n\nExtended help\n\nConvergence\n\nThe criterion used to determine convergence is that the relative change in the ELBO falls below the given rtol.\n\n\n\n\n\n","category":"method"},{"location":"methods/HistSmoother/#HistSmoother","page":"HistSmoother","title":"HistSmoother","text":"Documentation for the histogram smoother of Wand and Yu (2022).\n\nThis model is available through the BayesDensityHistSmoother package.","category":"section"},{"location":"methods/HistSmoother/#Module-API","page":"HistSmoother","title":"Module API","text":"","category":"section"},{"location":"methods/HistSmoother/#Evaluating-the-pdf-and-cdf","page":"HistSmoother","title":"Evaluating the pdf and cdf","text":"","category":"section"},{"location":"methods/HistSmoother/#Utility-functions","page":"HistSmoother","title":"Utility functions","text":"","category":"section"},{"location":"methods/HistSmoother/#Markov-chain-Monte-Carlo","page":"HistSmoother","title":"Markov chain Monte Carlo","text":"","category":"section"},{"location":"methods/HistSmoother/#Variational-inference","page":"HistSmoother","title":"Variational inference","text":"","category":"section"},{"location":"methods/HistSmoother/#BayesDensityHistSmoother.HistSmoother","page":"HistSmoother","title":"BayesDensityHistSmoother.HistSmoother","text":"HistSmoother{T<:Real} <: AbstractBayesDensityModel{T}\n\nStruct representing a spline histogram smoother model.\n\nConstructors\n\nHistSmoother(x::AbstractVector{<:Real}; kwargs...)\nHistSmoother{T}(x::AbstractVector{<:Real}; kwargs...)\n\nArguments\n\nx: The data vector.\n\nKeyword arguments\n\nK: B-spline basis dimension of a regular augmented spline basis. Defaults to 52.\nbounds: A tuple giving the support of the B-spline mixture model.\nn_bins: Number of bins used to construct the histogram likelihood. Defaults to 400.\nprior_scale_fixed: Scale hyperparameter for 0th and 1st order (fixed effect) spline terms. Defaults to 1000.0.\nprior_scale_random: Scale hyperparameter for the (higher order) mixed effects spline terms. Defaults to 1000.0.\n\nReturns\n\nshs: A histogram spline smoother object.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> shs = HistSmoother(x)\n52-dimensional HistSmoother{Float64}:\nUsing 5000 binned observations with 400 bins.\n support: (-0.105, 1.105)\nHyperparameters:\n prior_scale_fixed = 1000.0\n prior_scale_random = 1000.0\n\njulia> shs = HistSmoother(x; K = 80, prior_scale_fixed = 1e5);\n\nExtended help\n\nBinning\n\nThe binning step used by the spline histogram smoother is an essential part of the model fitting procedure, and can as such not be disabled. Using a greater number of bins means that less precision is lost due to the binning step, but makes the model fitting procedure slower due to a larger compuatational burden. Note that the number of bins only affects the model fitting process, and does otherwise not change the returned \n\nHyperparameter selection\n\nThe hyperparameter s_β contols the smoothness of the resulting density estimates. Setting this to a smaller value leads to smoother estimates.\n\n\n\n\n\n","category":"type"},{"location":"methods/HistSmoother/#Distributions.pdf-Union{Tuple{Vals}, Tuple{Names}, Tuple{HistSmoother, NamedTuple{Names, Vals}, Real}} where {Names, Vals<:Tuple}","page":"HistSmoother","title":"Distributions.pdf","text":"pdf(\n    shs::HistSmoother,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\npdf(\n    shs::HistSmoother,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate f(t  boldsymboleta) for a given HistSmoother when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain a field named :β. If the parameters argument does not contain a field named :norm, then the normalization constant will be computed using Simpson's method. Alternatively, if parameters contains the field :norm, then this value is used instead.\n\n\n\n\n\n","category":"method"},{"location":"methods/HistSmoother/#Distributions.cdf-Union{Tuple{Vals}, Tuple{Names}, Tuple{HistSmoother, NamedTuple{Names, Vals}, Real}} where {Names, Vals<:Tuple}","page":"HistSmoother","title":"Distributions.cdf","text":"cdf(\n    shs::HistSmoother,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\ncdf(\n    shs::HistSmoother,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate F(t  boldsymboleta) for a given HistSmoother when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain a field named :β. If the parameters argument does not contain a field named :norm, then the normalization constant will be computed using Simpson's method. Alternatively, if parameters contains the field :norm, then this value is used instead.\n\nInternally, this function computes the cdf on a predefined regular grid, and uses linear interpolation to approximate the cdf.\n\n\n\n\n\n","category":"method"},{"location":"methods/HistSmoother/#BayesDensityCore.hyperparams-Tuple{HistSmoother}","page":"HistSmoother","title":"BayesDensityCore.hyperparams","text":"hyperparams(\n    shs::HistSmoother{T}\n) where {T} -> @NamedTuple{prior_scale_fixed::T, prior_scale_random::T}\n\nReturns the hyperparameters of the spline histogram smoother shs as a NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"methods/HistSmoother/#StatsBase.sample-Tuple{AbstractRNG, HistSmoother, Int64}","page":"HistSmoother","title":"StatsBase.sample","text":"sample(\n    [rng::Random.AbstractRNG],\n    hs::HistSmoother{T},\n    n_samples::Int;\n    n_burnin::Int              = min(100, div(n_samples, 5)),\n    initial_params::NamedTuple = get_default_initparams_mcmc(hs)\n) where {T} -> PosteriorSamples{T}\n\nGenerate n_samples posterior samples from a HistSmoother using an augmented Gibbs sampler.\n\nArguments\n\nrng: Optional random seed used for random variate generation.\nhs: The HistSmoother object for which posterior samples are generated.\nn_samples: The total number of samples (including burn-in).\n\nKeyword arguments\n\nn_burnin: Number of burn-in samples.\ninitial_params: Initial values used in the MCMC algorithm. Should be supplied as a NamedTuple with fields :β and :σ2, where :β is a K-dimensional vector and σ2 is a positive scalar.\n\nReturns\n\nps: A PosteriorSamples object holding the posterior samples and the original model object.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> hs = HistSmoother(x);\n\njulia> ps = sample(Xoshiro(1), hs, 1100);\n\n\n\n\n\n","category":"method"},{"location":"methods/HistSmoother/#BayesDensityHistSmoother.HistSmootherVIPosterior","page":"HistSmoother","title":"BayesDensityHistSmoother.HistSmootherVIPosterior","text":"HistSmootherVIPosterior{T<:Real} <: AbstractVIPosterior{T}\n\nStruct representing the variational posterior distribution of a HistSmoother.\n\nFields\n\nq_β: Distribution representing the optimal variational density q*(β).\nq_σ: Distribution representing the optimal variational density q*(σ²).\nshs: The HistSmoother to which the variational posterior was fit.\n\n\n\n\n\n","category":"type"},{"location":"methods/HistSmoother/#BayesDensityCore.varinf-Tuple{HistSmoother}","page":"HistSmoother","title":"BayesDensityCore.varinf","text":"varinf(\n    hs::HistSmoother{T};\n    initial_params::NamedTuple = _get_default_initparams_varinf(hs),\n    max_iter::Int              = 500,\n    rtol::Real                 = 1e-5\n) where {T} -> HistSmootherVIPosterior{T}\n\nFind a variational approximation to the posterior distribution of a HistSmoother using mean-field variational inference.\n\nArguments\n\nhs: The HistSmoother whose posterior we want to approximate.\n\nKeyword arguments\n\ninitial_params: Initial values of the VI parameters μ_opt, Σ_opt and b_σ_opt.\nmax_iter: Maximal number of VI iterations. Defaults to 500.\nrtol: Relative tolerance used to determine convergence. Defaults to 1e-5.\n\nReturns\n\nvip: A HistSmootherVIPosterior object representing the variational posterior.\ninfo: A VariationalOptimizationResult describing the result of the optimization.\n\nnote: Note\nTo run the optimization loop for a fixed number of iterations irrespective of the convergence criterion, one can set rtol = 0.0, and max_iter equal to the desired total iteration count. Note that setting rtol to a strictly negative value will issue a warning.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> hs = HistSmoother(x);\n\njulia> vip, info = varinf(hs; rtol=1e-6);\n\nExtended help\n\nConvergence\n\nThe criterion used to determine convergence is that the relative change in the expectation of mathbbE(sigma^-2) falls below the given rtol.\n\n\n\n\n\n","category":"method"},{"location":"methods/PitmanYorMixture/#PitmanYorMixture","page":"PitmanYorMixture","title":"PitmanYorMixture","text":"Documentation for Pitman-Yor mixture models Ishwaran and James (2001), with a normal kernel and a normal-inverse gamma base measure.\n\nFor Markov chain Monte Carlo based inference, this module implements algorithm 2 by Neal (2000). For variational inference, we implement the truncated stickbreaking approach of Blei and Jordan (2006).\n\nnote: Note\nSince Dirichlet process mixture models are equivalent to a Pitman-Yor mixture model with discount parameter equal to 0, this module can also be used to fit the former type of models.","category":"section"},{"location":"methods/PitmanYorMixture/#Module-API","page":"PitmanYorMixture","title":"Module API","text":"","category":"section"},{"location":"methods/PitmanYorMixture/#Evaluating-the-pdf-and-cdf","page":"PitmanYorMixture","title":"Evaluating the pdf and cdf","text":"","category":"section"},{"location":"methods/PitmanYorMixture/#Utility-functions","page":"PitmanYorMixture","title":"Utility functions","text":"","category":"section"},{"location":"methods/PitmanYorMixture/#Markov-chain-Monte-Carlo","page":"PitmanYorMixture","title":"Markov chain Monte Carlo","text":"","category":"section"},{"location":"methods/PitmanYorMixture/#Variational-inference","page":"PitmanYorMixture","title":"Variational inference","text":"","category":"section"},{"location":"methods/PitmanYorMixture/#BayesDensityPitmanYorMixture.PitmanYorMixture","page":"PitmanYorMixture","title":"BayesDensityPitmanYorMixture.PitmanYorMixture","text":"PitmanYorMixture{T<:Real} <: AbstractBayesDensityModel{T}\n\nStruct representing a Pitman-Yor mixture model with a normal kernel and a conjugate Normal-InverseGamma base measure.\n\nConstructors\n\nPitmanYorMixture(x::AbstractVector{<:Real}; kwargs...)\nPitmanYorMixture{T}(x::AbstractVector{<:Real}; kwargs...)\n\nArguments\n\nx: The data vector.\n\nKeyword arguments\n\ndiscount: Discount parameter of the Pitman-Yor process. Defaults to 0.0, corresponding to a Dirichlet Process.\nstrength: Strength parameter of the Pitman-Yor process. Defaults to 1.0.\nprior_location: Prior mean of the location parameter μ. Defaults to mean(x).\nprior_inv_scale_fac: Factor by which the conditional prior variance σ2 of μ is scaled. Defaults to 1.\nprior_shape: Prior shape parameter of the squared scale parameter σ2: Defaults to 2.0.\nprior_rate: Prior rate parameter of the squared scale parameter σ2. Defaults to var(x).\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> pym = PitmanYorMixture(x)\nPitmanYorMixture{Float64}:\nUsing 5000 observations.\nHyperparameters:\n discount = 0.0, strength = 1.0\n prior_location = 0.578555, prior_inv_scale_fac = 1.0\n prior_shape = 2.0, prior_rate = 0.0334916\n\njulia> pym = PitmanYorMixture(x; strength = 2, discount = 0.5);\n\nExtended help\n\n\n\n\n\n","category":"type"},{"location":"methods/PitmanYorMixture/#Distributions.pdf-Tuple{PitmanYorMixture, NamedTuple, Real}","page":"PitmanYorMixture","title":"Distributions.pdf","text":"pdf(\n    bsm::PitmanYorMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\npdf(\n    bsm::PitmanYorMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate f(t  boldsymboleta) for a given PitmanYorMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain fields named :μ, :σ2 and a third field named :cluster_counts or :w, depending on whether the marginal or stickbreaking parameterization is used.\n\n\n\n\n\n","category":"method"},{"location":"methods/PitmanYorMixture/#Distributions.cdf-Tuple{PitmanYorMixture, NamedTuple, Real}","page":"PitmanYorMixture","title":"Distributions.cdf","text":"cdf(\n    bsm::PitmanYorMixture,\n    params::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\ncdf(\n    bsm::PitmanYorMixture,\n    params::AbstractVector{NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate F(t  boldsymboleta) for a given PitmanYorMixture when the model parameters of the NamedTuple params are given by boldsymboleta.\n\nThe named tuple should contain fields named :μ, :σ2 and a third field named :cluster_counts or :w, depending on whether the marginal or stickbreaking parameterization is used.\n\n\n\n\n\n","category":"method"},{"location":"methods/PitmanYorMixture/#BayesDensityCore.hyperparams-Tuple{PitmanYorMixture}","page":"PitmanYorMixture","title":"BayesDensityCore.hyperparams","text":"hyperparams(\n    pym::PitmanYorMixture{T}\n) where {T} -> @NamedTuple{discount::T, strength::T, prior_location::T, prior_inv_scale_fac::T, prior_shape::T, prior_rate::T}\n\nReturns the hyperparameters of the Pitman-Yor mixture model pym as a NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"methods/PitmanYorMixture/#StatsBase.sample-Tuple{AbstractRNG, PitmanYorMixture, Int64}","page":"PitmanYorMixture","title":"StatsBase.sample","text":"sample(\n    [rng::Random.AbstractRNG],\n    pym::PitmanYorMixture{T},\n    n_samples::Int;\n    n_burnin::Int              = min(1000, div(n_samples, 5)),\n    initial_params::NamedTuple = _get_default_initparams_mcmc(pym)\n) where {T} -> PosteriorSamples{T}\n\nGenerate n_samples posterior samples from a PitmanYorMixture using an augmented marginal Gibbs sampler.\n\nArguments\n\nrng: Optional random seed used for random variate generation.\npym: The PitmanYorMixture object for which posterior samples are generated.\nn_samples: The total number of samples (including burn-in).\n\nKeyword arguments\n\nn_burnin: Number of burn-in samples.\ninitial_params: Initial values used in the MCMC algorithm. Should be supplied as a NamedTuple with fields :μ, :σ2 and :cluster_alloc, where μ and σ2 are vector of the same dimension, and cluster_alloc is vector of length length(x) indicating the cluster membership of each observation.\n\nReturns\n\nps: A PosteriorSamples object holding the posterior samples and the original model object.\n\nExamples\n\njulia> using Random\n\njulia> x = (1.0 .- (1.0 .- LinRange(0.0, 1.0, 5000)) .^(1/3)).^(1/3);\n\njulia> pym = PitmanYorMixture(x);\n\njulia> ps = sample(Xoshiro(1), model, 5000);\n\njulia> ps = sample(Xoshiro(1), model, 5000; initial_params = (μ = [0.2, 0.8], σ2 = [1.0, 1.0], cluster_alloc = vcat(fill(1, 2500), fill(2, 2500))));\n\n\n\n\n\n","category":"method"},{"location":"methods/PitmanYorMixture/#BayesDensityPitmanYorMixture.PitmanYorMixtureVIPosterior","page":"PitmanYorMixture","title":"BayesDensityPitmanYorMixture.PitmanYorMixtureVIPosterior","text":"PitmanYorMixtureVIPosterior{T<:Real} <: AbstractVIPosterior{T}\n\nStruct representing the variational posterior distribution of a PitmanYorMixture.\n\nFields\n\nq_v: Vector of distributions representing the optimal variational densities q*(vₖ), i.e. the density of the stick-breaking weights.\nq_θ: Vector of distributions representing the optimal variational densities q*(θₖ), i.e. the joint density of the mixture component means and variances.\npym: The PitmanYorMixture to which the variational posterior was fit.\n\n\n\n\n\n","category":"type"},{"location":"methods/PitmanYorMixture/#BayesDensityCore.varinf-Tuple{PitmanYorMixture}","page":"PitmanYorMixture","title":"BayesDensityCore.varinf","text":"varinf(\n    pym::PitmanYorMixture{T};\n    truncation_level::Int      = 25,\n    initial_params::NamedTuple = _get_default_initparams(pym, truncation_level),\n    max_iter::Int              = 3000\n    rtol::Real                 = 1e-6\n) where {T} -> PitmanYorMixtureVIPosterior{T}\n\nFind a variational approximation to the posterior distribution of a PitmanYorMixture using mean-field variational inference based on a truncated stickbreaking-approach.\n\nArguments\n\npym: The PitmanYorMixture whose posterior we want to approximate.\n\nKeyword arguments\n\ntruncation level: Positive integer specifying the truncation level of the variational approximation. Defaults to 25.\ninitial_params: Initial values of the VI parameters a_v b_v, locations and inv_scale_facs, shapes and rates, supplied as a NamedTuple. Must have dimensions matching the supplied truncation level.\nmax_iter: Maximal number of VI iterations. Defaults to 3000.\nrtol: Relative tolerance used to determine convergence. Defaults to 1e-6.\n\nReturns\n\nvip: A PitmanYorMixtureVIPosterior object representing the variational posterior.\ninfo: A VariationalOptimizationResult describing the result of the optimization.\n\nnote: Note\nTo perform the optimization for a fixed number of iterations irrespective of the convergence criterion, one can set rtol = 0.0, and max_iter equal to the desired total iteration count. Note that setting rtol to a strictly negative value will issue a warning.\n\nExtended help\n\nConvergence\n\nThe criterion used to determine convergence is that the relative change in the ELBO falls below the given rtol.\n\nTruncation\n\nThe truncation level determines the maximal number of components used in the variational approximation. Generally, setting the truncation level to a higher value leads to an approximating class with a greater representational capacity, at the cost of increased computation.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#General-API","page":"General API","title":"General API","text":"This page explains how to fit the Bayesian density models implemented in BayesDensity.jl. Most of the methods implemented in this package support two modes of posterior inference: simulation consistent inference through Markov chain Monte Carlo (MCMC) and approximate through variational inference (VI). We also document most of the convenience methods available for computing select posterior quantities of interest, such as the posterior mean or quantiles of f(t) for some t in mathbbR.\n\nThe plotting API of this package is documented on a separate page","category":"section"},{"location":"api/general_api/#Defining-models","page":"General API","title":"Defining models","text":"The first step to estimating a density with this package is to create a model object for which posterior inference is desired. All density models in this package are subtypes of AbstractBayesDensityModel:\n\nIn order to create a model object, we call the corresponding contructor with the data and other positional- and keyword arguments. For example, we can create a BSplineMixture object with default hyperparameters as follows:\n\nbsm = BSplineMixture(randn(1000))\nnothing # hide\n\nFor more detailed information on the arguments supported by each specific Bayesian density model we refer the reader to the methods documentation.","category":"section"},{"location":"api/general_api/#Evaluating-the-density-and-the-cumulative-distribution-function","page":"General API","title":"Evaluating the density and the cumulative distribution function","text":"The density estimators implemented in this package all specify a model f(t boldsymboleta) for the density of the data, which depends on a parameter boldsymboleta. In order to calculate f(cdot) for a given boldsymboleta, each Bayesian density model implements the pdf method.\n\nFor models that only implement the signature pdf(::AbstractBayesDensityModel, ::Any, ::Real), a generic fallback method is provided for vectors of parameters and vector evaluation grids. However, it is recommended that most models provide specialized methods for vectors of parameters and vectors of evaluation points, as it is often possible to implement batch evaluation more efficiently (e.g. by leveraging BLAS calls instead of loops) when the parameters and the evaluation grid are provided in batches.\n\nThe cumulative distribution function of a model can be computed in a similar way by using the cdf method:\n\nGeneric fallback methods for computing the cdf for vectors of parameters and vector evaluation grids are also provided for models that implement the signature cdf(::AbstractBayesDensityModel, ::Any, ::Real).","category":"section"},{"location":"api/general_api/#Other-methods","page":"General API","title":"Other methods","text":"All of the density models implemented in this package depend on the choice of various hyperparameters, which can be retrieved by utilizing the following method:\n\nFor the exact format of the returned hyperparameters for a specific Bayesian density model type, we refer to the docs of the individual density estimators.\n\nTo compute the support of a given model, the support method is provided.\n\nThe element type of the model object can be determined via the eltype method:\n\nThe following function is used to select a grid used for plotting of fitted model objects. A default fallback is provided, but it may be necessary to overload this method when implementing new models in order to make plotting functions work without having to supply an explicit grid of values for the first coordinate axis.","category":"section"},{"location":"api/general_api/#Markov-chain-Monte-Carlo","page":"General API","title":"Markov chain Monte Carlo","text":"The main workhorse of MCMC-based inference is the sample method, which takes a Bayesian density model object as input and generates posterior samples through a specialized MCMC routine.\n\nAll of the implemented MCMC methods return an object of type PosteriorSamples:\n\nThe following methods can be used to extract useful information about the model object, such as the underlying model object and the number of samples.\n\nBy default, PosteriorSamples objects also store the burn-in samples from the MCMC routine. These can be discarded via the following method:\n\nMultiple PosteriorSamples objects can also be concatenated to create a single PosteriorSamples object. This is particularly useful when a preliminary MCMC run is deemed to be too short, and one wants to pool the original samples with the samples from a new MCMC run.","category":"section"},{"location":"api/general_api/#Computing-posterior-summary-statistics","page":"General API","title":"Computing posterior summary statistics","text":"When using Bayesian density estimators, we are often interested in computing various summary statistics of the posterior draws from an MCMC procedure. For instance, we may be interested in providing an estimate of the density f (e.g. the posterior mean) and to quantify the uncertainty in this estimate (e.g. via credible bands).\n\nTo this end, BayesDensityCore provides methods for PosteriorSamples objects that let us easily compute relevant summary statistics for the density f, as shown in the short example below:\n\nbsm = BSplineMixture(randn(1000))\nposterior = sample(bsm, 2000; n_burnin=400)\n\n# Compute the posterior mean of f(0.5)\nmean(posterior, pdf, 0.5)\n\n# Compute the posterior 0.05 and 0.95-quantiles of f(0.5)\n# Note that supplying pdf as the second argument is optional here\nquantile(posterior, 0.5, [0.05, 0.95]) == quantile(posterior, pdf, 0.5, [0.05, 0.95])\nnothing # hide\n\nIn some cases it may also be of interest to carry out posterior inference for the cumulative distribution function F(t) = int_-infty^t f(s) textds. Computing posterior summary statistics for the cdf instead of the pdf is easily achieved by replacing the pdf in the second argument with cdf instead:\n\n# Compute the posterior mean of F(0.5)\nmean(posterior, cdf, 0.5)\n\n# Compute the posterior 0.05 and 0.95-quantiles of F(0.5)\n# Note that supplying cdf as the second argument is necessary here\nquantile(posterior, cdf, 0.5, [0.05, 0.95])\nnothing # hide\n\nThe posterior summary statistics available through BayesDensityCore are the following:","category":"section"},{"location":"api/general_api/#Variational-inference","page":"General API","title":"Variational inference","text":"The varinf method can be used to compute a variational approximation to the posterior distribution:\n\nAny call to varinf will return a subtype of the abstract type AbstractVIPosterior:\n\nFor most models, varinf also returns an object which stores the result of the optimization procedure, see VariationalOptimizationResult.\n\nThe following convenience methods are also part of the public API:","category":"section"},{"location":"api/general_api/#Generating-samples-from-the-variational-posterior","page":"General API","title":"Generating samples from the variational posterior","text":"The sample method makes it possible to generate independent samples from the variational posterior. This is particularly useful in cases where inference for multiple posterior quantities (e.g. medians, variances) is desired.\n\nAs shown in the above docstring, using the sample method on a AbstractVIPosterior object returns an object of type PosteriorSamples. As such, all of the convenience methods showcased in the previous subsection will also work for the object returned by sample.","category":"section"},{"location":"api/general_api/#Computing-posterior-summary-statistics-2","page":"General API","title":"Computing posterior summary statistics","text":"BayesDensityCore also provides convenience methods for AbstractVIPosterior objects that let us easily compute relevant summary statistics for the density f and the cdf F directly from the variational posterior object:\n\nbsm = BSplineMixture(randn(1000))\nviposterior, info = varinf(bsm)\n\n# Compute the (variational) posterior mean of f(0.5)\nmean(viposterior, pdf, 0.5)\n\n# Compute the (variational) posterior median of F(0.5)\nmedian(viposterior, cdf, 0.5)\n\n# Compute the (variational) posterior 0.05 and 0.95-quantiles of f(0.5)\n# Note that supplying pdf as the second argument is optional here\nquantile(viposterior, 0.5, [0.05, 0.95]) ≈ quantile(viposterior, pdf, 0.5, [0.05, 0.95])\nnothing # hide\n\nThe full list of available summary statistics is the same as that for PosteriorSamples objects:\n\nnote: Note\nNote that each call to mean, quantile, median, var or std in most cases will first simulate a random sample from the posterior distribution, and then uses this sample to compute a Monte Carlo approximation of the quantity of interest using these samples. If posterior inference for multiple quantities is desired, then it is recommended to first use sample, and call these functions on this object as only a single batch of posterior samples is generated in this case.","category":"section"},{"location":"api/general_api/#Storing-info-from-the-variational-optimization","page":"General API","title":"Storing info from the variational optimization","text":"In order to provide a simple way of performing convergence diagnostics for variational optimization problems, BayesDensityCore exports the VariationalOptimizationResult type.","category":"section"},{"location":"api/general_api/#BayesDensityCore.AbstractBayesDensityModel","page":"General API","title":"BayesDensityCore.AbstractBayesDensityModel","text":"AbstractBayesDensityModel{T<:Real}\n\nAbstract super type for all Bayesian density models implemented in this package.\n\n\n\n\n\n","category":"type"},{"location":"api/general_api/#Distributions.pdf-Tuple{AbstractBayesDensityModel, Any, Real}","page":"General API","title":"Distributions.pdf","text":"pdf(\n    bdm::AbstractBayesDensityModel,\n    parameters::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\npdf(\n    bdm::AbstractBayesDensityModel,\n    parameters::AbstractVector{<:NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate f(t boldsymboleta) of the Bayesian density model bdm for every boldsymboleta in parameters and every element in the collection t.\n\nIf a single NamedTuple is passed to the parameters argument, this function outputs either a scalar or a vector depending on the input type of the third argument t.\n\nIf a vector of NamedTuples is passed to the second positional argument, then this function returns a Matrix of size (length(t), length(parameters)).\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Distributions.cdf-Tuple{AbstractBayesDensityModel, Any, Real}","page":"General API","title":"Distributions.cdf","text":"cdf(\n    bdm::AbstractBayesDensityModel,\n    parameters::NamedTuple,\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\ncdf(\n    bdm::AbstractBayesDensityModel,\n    parameters::AbstractVector{<:NamedTuple},\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Matrix{<:Real}\n\nEvaluate the cumulative distribution function F(t boldsymboleta) = int_-infty^t f(sboldsymboleta)textds of the Bayesian density model bdm for every boldsymboleta in parameters and every element in the collection t.\n\nIf a single NamedTuple is passed to the parameters argument, this function outputs either a scalar or a vector depending on the input type of the third argument t.\n\nIf a vector of NamedTuples is passed to the second positional argument, then this function returns a Matrix of size (length(t), length(parameters)).\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.hyperparams-Tuple{AbstractBayesDensityModel}","page":"General API","title":"BayesDensityCore.hyperparams","text":"hyperparams(bdm::AbstractBayesDensityModel) -> @NamedTuple\n\nReturn the hyperparameters of the model bdm as a NamedTuple.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Distributions.support-Union{Tuple{AbstractBayesDensityModel{T}}, Tuple{T}} where T","page":"General API","title":"Distributions.support","text":"support(bdm::AbstractBayesDensityModel{T}) where {T} -> NTuple{2, T}\n\nReturn the support of the model bdm as an 2-dimensional tuple.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Base.eltype-Union{Tuple{AbstractBayesDensityModel{T}}, Tuple{T}} where T","page":"General API","title":"Base.eltype","text":"eltype(::AbstractBayesDensityModel{T}) where {T}\n\nReturn the element type of a Bayesian density model.\n\n\n\n\n\neltype(::PosteriorSamples{T}) where {T}\n\nGet the element type of a PosteriorSamples object.\n\n\n\n\n\neltype(::AbstractVIPosterior{T}) where {T}\n\nGet the element type of a variational posterior object.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.default_grid_points","page":"General API","title":"BayesDensityCore.default_grid_points","text":"default_grid_points(bdm::AbstractBayesDensityModel{T}) where {T} -> AbstractVector{T}\n\nGet the default grid used for plotting of density estimates.\n\nDefaults to returning constructing a grid based on the extrema of bdm.data.x. If a given struct does not store a copy of the original data used to construct the model object as bdm.data.x, this method should be implemented.\n\n\n\n\n\n","category":"function"},{"location":"api/general_api/#StatsBase.sample-Tuple{AbstractBayesDensityModel, Int64}","page":"General API","title":"StatsBase.sample","text":"sample(\n    [rng::Random.AbstractRNG],\n    bdm::AbstractBayesDensityModel{T},\n    n_samples::Int\n    args...;\n    [n_burnin::Int],\n    kwargs...\n) where {T} -> PosteriorSamples{T}\n\nGenerate approximate posterior samples from the density model bdm using Markov chain Monte Carlo methods.\n\nThis functions returns a PosteriorSamples object which can be used to compute posterior quantities of interest such as the posterior mean of f(t) or posterior quantiles. See the specific method docstring for method-specific additional positional and keyword arguments.\n\nArguments\n\nrng: Seed used for random variate generation.\nbdm: The Bayesian density model object to generate posterior samples from.\nn_samples: Number of Monte Carlo samples (including burn-in)\nargs...: Other model-specific positional arguments. See the specific model docstring for this method for further details.\n\nKeyword arguments\n\nn_burnin: Number of burn-in samples.\nkwargs...: Other model-specific kwargs. See the specific model docstring for this method for further details.\n\nReturns\n\nps: A PosteriorSamples object holding the posterior samples and the original model object.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.PosteriorSamples","page":"General API","title":"BayesDensityCore.PosteriorSamples","text":"PosteriorSamples{T<:Real}\n\nStruct holding posterior samples of the parameters of a Bayesian density model.\n\nFields\n\nsamples: Vector holding posterior samples of model parameters.\nmodel: The model object to which samples were fit.\nn_samples: Total number of Monte Carlo samples. \nnon_burnin_ind: Indices of non-burnin samples.\n\n\n\n\n\n","category":"type"},{"location":"api/general_api/#BayesDensityCore.samples-Tuple{PosteriorSamples}","page":"General API","title":"BayesDensityCore.samples","text":"samples(ps::PosteriorSamples{T, V}) where {T, V<:AbstractVector} -> V\n\nGet the posterior samples of a PosteriorSamples object as a vector.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.model-Tuple{PosteriorSamples}","page":"General API","title":"BayesDensityCore.model","text":"model(ps::PosteriorSamples) -> AbstractBayesDensityModel\n\nReturn the model object of ps.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.n_samples-Tuple{PosteriorSamples}","page":"General API","title":"BayesDensityCore.n_samples","text":"n_samples(ps::PosteriorSamples) -> Int\n\nGet the total number of samples of a PosteriorSamples object, including burn-in samples.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.n_burnin-Tuple{PosteriorSamples}","page":"General API","title":"BayesDensityCore.n_burnin","text":"n_burnin(ps::PosteriorSamples) -> Int\n\nGet the number of burn-in samples of a PosteriorSamples object.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.drop_burnin-Tuple{PosteriorSamples}","page":"General API","title":"BayesDensityCore.drop_burnin","text":"drop_burnin(ps::PosteriorSamples{T}) where {T} -> PosteriorSamples{T}\n\nCreate a new PosteriorSamples object where the burn-in samples have been discarded.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Base.vcat-Tuple{Vararg{PosteriorSamples}}","page":"General API","title":"Base.vcat","text":"vcat(ps::PosteriorSamples...) -> PosteriorSamples\n\nConcatenate the samples of multiple PosteriorSamples objects to form a single object.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.mean-Tuple{PosteriorSamples}","page":"General API","title":"Statistics.mean","text":"mean(\n    ps::PosteriorSamples,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\nCompute the approximate posterior mean of a functional of f for every element in the collection t using Monte Carlo samples.\n\nThe target functional can be either be the pdf f or the cdf F, and is controlled by adjusting the func argument. By default, the posterior mean of f is computed.\n\nIf the input t is a scalar, a scalar is returned. If t is a vector, this function returns a vector the same length as t.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0, 1, 5001)) .^(1/3)).^(1/3);\n\njulia> ps = sample(Random.Xoshiro(1), BSplineMixture(x), 5000);\n\njulia> mean(ps, 0.1)\n0.0969450407517681\n\njulia> mean(ps, [0.1, 0.8])\n2-element Vector{Float64}:\n 0.0969450407517681\n 1.3662358915400654\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.quantile-Tuple{PosteriorSamples}","page":"General API","title":"Statistics.quantile","text":"quantile(\n    ps::PosteriorSamples,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}},\n    q::RealAbstractVector{<:Real,\n) -> Union{Real, Vector{<:Real}}\n\nquantile(\n    ps::PosteriorSamples,\n    [func = pdf]\n    t::Union{Real, AbstractVector{<:Real}},\n    q::AbstractVector{<:Real},\n) -> Matrix{<:Real}\n\nCompute the approximate posterior quantile(s) of a functional of f for every element in the collection t using Monte Carlo samples.\n\nThe target functional can be either be the pdf f or the cdf F, and is controlled by adjusting the func argument. By default, the posterior quantiles of f are computed.\n\nIn the case where both t and q are scalars, the output is a real number. When t is a vector and q a scalar, this function returns a vector of the same length as t. If q is supplied as a Vector, then this method returns a Matrix of dimension (length(t), length(q)), where each column corresponds to a given quantile. This is also the case when t is supplied as a scalar.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0, 1, 5001)) .^(1/3)).^(1/3);\n\njulia> ps = sample(Random.Xoshiro(1), BSplineMixture(x), 5000);\n\njulia> quantile(ps, 0.1, 0.5); # Get the posterior median of f(0.5)\n\njulia> quantile(ps, cdf, 0.1, 0.5); # Get the posterior median of F(0.5)\n\njulia> quantile(ps, [0.1, 0.8], [0.05, 0.95]); # Get the posterior 0.05, 0.95-quantiles of f(0.1) and f(0.8)\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.median-Tuple{PosteriorSamples}","page":"General API","title":"Statistics.median","text":"median(\n    ps::PosteriorSamples,\n    [func = pdf]\n    t::Union{Real, AbstractVector{<:Real}},\n) -> Union{Real, Vector{<:Real}}\n\nCompute the approximate posterior median of a functional of f for every element in the collection t using Monte Carlo samples.\n\nThe target functional can be either be the pdf f or the cdf F, and is controlled by adjusting the func argument. By default, the posterior median of f is computed.\n\nIf the input t is a scalar, a scalar is returned. If t is a vector, this function returns a vector the same length as t.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.var-Tuple{PosteriorSamples}","page":"General API","title":"Statistics.var","text":"var(\n    ps::PosteriorSamples,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\nCompute the approximate posterior variance of a functional of f for every element in the collection t using Monte Carlo samples.\n\nThe target functional can be either be the pdf f or the cdf F, and is controlled by adjusting the func argument. By default, the posterior variance of f is computed.\n\nIf the input t is a scalar, a scalar is returned. If t is a vector, this function returns a vector the same length as t.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0, 1, 5001)) .^(1/3)).^(1/3);\n\njulia> ps = sample(Random.Xoshiro(1), BSplineMixture(x), 5000);\n\njulia> var(ps, 0.1) # get the posterior variance of f(0.1)\n0.00027756364767372627\n\njulia> var(ps, [0.1, 0.8]) # get the posterior variance of f(0.1) and f(0.8)\n2-element Vector{Float64}:\n 0.00027756364767372627\n 0.005977674286240125\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.std-Tuple{PosteriorSamples}","page":"General API","title":"Statistics.std","text":"std(\n    ps::PosteriorSamples,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}}\n) -> Union{Real, Vector{<:Real}}\n\nCompute the approximate posterior standard deviation of a functional of f for every element in the collection t using Monte Carlo samples.\n\nThe target functional can be either be the pdf f or the cdf F, and is controlled by adjusting the func argument. By default, the posterior standard deviation of f is computed.\n\nIf the input t is a scalar, a scalar is returned. If t is a vector, this function returns a vector the same length as t. This method is equivalent to sqrt.(var(rng, vip, t, n_samples)).\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.varinf-Tuple{AbstractBayesDensityModel}","page":"General API","title":"BayesDensityCore.varinf","text":"varinf(\n    bdm::AbstractBayesDensityModel{T},\n    args...;\n    kwargs...\n) where {T}\n\nCompute a variational approximation to the posterior distribution.\n\nThe positional arguments and keyword arguments supported by this function, as well as the type of the returned variational posterior object differs between different subtypes of AbstractBayesDensityModel.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.AbstractVIPosterior","page":"General API","title":"BayesDensityCore.AbstractVIPosterior","text":"AbstractVIPosterior{T<:Real}\n\nAbstract super type representing the variational posterior distribution of AbstractBayesDensityModel\n\n\n\n\n\n","category":"type"},{"location":"api/general_api/#BayesDensityCore.model-Tuple{AbstractVIPosterior}","page":"General API","title":"BayesDensityCore.model","text":"model(vip::AbstractVIPosterior{T}) where {T} -> AbstractBayesDensityModel{T}\n\nGet the model object to which the variational posterior vip was fitted.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Base.eltype-Tuple{AbstractVIPosterior}","page":"General API","title":"Base.eltype","text":"eltype(::AbstractBayesDensityModel{T}) where {T}\n\nReturn the element type of a Bayesian density model.\n\n\n\n\n\neltype(::PosteriorSamples{T}) where {T}\n\nGet the element type of a PosteriorSamples object.\n\n\n\n\n\neltype(::AbstractVIPosterior{T}) where {T}\n\nGet the element type of a variational posterior object.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#StatsBase.sample-Tuple{AbstractVIPosterior, Int64}","page":"General API","title":"StatsBase.sample","text":"sample(\n    [rng::Random.AbstractRNG],\n    vip::AbstractVIPosterior{T},\n    n_samples::Int\n) where {T} -> PosteriorSamples{T}\n\nGenerate n_samples independent samples from the variationonal posterior distribution vip.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0, 1, 5001)) .^(1/3)).^(1/3);\n\njulia> vip = varinf(BSplineMixture(x));\n\njulia> vps = sample(Random.Xoshiro(1812), vip, 5000);\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.mean-Tuple{AbstractVIPosterior}","page":"General API","title":"Statistics.mean","text":"mean(\n    [rng::Random.AbstractRNG],\n    vip::AbstractVIPosterior,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}},\n    [n_samples::Int=1000]\n) -> Union{Real, Vector{<:Real}}\n\nCompute the approximate posterior mean of f(t) for every element in the collection t using Monte Carlo samples.\n\nIf the input t is a scalar, a scalar is returned. If t is a vector, this function returns a vector the same length as t.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0, 1, 5001)) .^(1/3)).^(1/3);\n\njulia> vip = varinf(BSplineMixture(x));\n\njulia> mean(Random.Xoshiro(1), vip, 0.1)\n0.08615412808594237\n\njulia> mean(Random.Xoshiro(1), vip, [0.1, 0.8])\n2-element Vector{Float64}:\n 0.08615412808594237\n 1.3674886390998342\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.quantile-Tuple{AbstractVIPosterior}","page":"General API","title":"Statistics.quantile","text":"quantile(\n    [rng::Random.AbstractRNG],\n    vip::AbstractVIPosterior,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}},\n    q::Union{Real, AbstractVector{<:Real}},\n    [n_samples::Int=1000]\n) -> Union{Real, Vector{<:Real}}\n\nquantile(\n    [rng::Random.AbstractRNG],\n    vip::AbstractVIPosterior,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}},\n    q::AbstractVector{<:Real},\n    [n_samples::Int=1000]\n) -> Matrix{<:Real}\n\nCompute the posterior q-quantile(s) of the pdf f(t) or the cdf F(t) for each element in the collection t.\n\nBy default this function falls back to quantile(sample(rng, vip, n_samples), func, t, q)\n\nIn the case where both t and q are scalars, the output is a real number. When t is a vector and q a scalar, this function returns a vector of the same length as t. If q is supplied as a Vector, then this function returns a Matrix of dimension (length(t), length(q)), where each column corresponds to a given quantile. This is also the case when t is supplied as a scalar.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0, 1, 5001)) .^(1/3)).^(1/3);\n\njulia> vip = varinf(BSplineMixture(x));\n\njulia> quantile(Random.Xoshiro(1), vip, 0.9, 0.5)\n0.537450082172813\n\njulia> quantile(Random.Xoshiro(1), vip, [0.2, 0.8], [0.05, 0.95])\n2×2 Matrix{Float64}:\n 0.34042  0.362478\n 1.3039   1.43599\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.median-Tuple{AbstractVIPosterior}","page":"General API","title":"Statistics.median","text":"median(\n    [rng::Random.AbstractRNG],\n    vip::AbstractVIPosterior,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}},\n    [n_samples::Int=1000]\n) -> Union{Real, Vector{<:Real}}\n\nCompute the posterior median of the pdf f(t) or the cdf F(t) for each element in the collection t.\n\nEquivalent to quantile(rng, vip, t, 0.5, n_samples).\n\nIn the case where both t and q are scalars, the output is a real number. When t is a vector, this function returns a vector of the same length as t.\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.var-Tuple{AbstractVIPosterior}","page":"General API","title":"Statistics.var","text":"var(\n    [rng::Random.AbstractRNG],\n    vip::AbstractVIPosterior,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}},\n    [n_samples::Int=1000]\n) -> Union{Real, Vector{<:Real}}\n\nCompute the posterior variance of the pdf f(t) or the cdf F(t) for each element in the collection t.\n\nIf the input t is a scalar, a scalar is returned. If t is a vector, this function returns a vector the same length as t.\n\nExamples\n\njulia> x = (1.0 .- (1.0 .- LinRange(0, 1, 5001)) .^(1/3)).^(1/3);\n\njulia> vip = varinf(BSplineMixture(x));\n\njulia> var(Random.Xoshiro(1), vip, 0.1)\n3.2895012264929507e-6\n\njulia> var(Random.Xoshiro(1), vip, [0.1, 0.8])\n2-element Vector{Float64}:\n 3.2895012264929507e-6\n 0.0014793006688108977\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#Statistics.std-Tuple{AbstractVIPosterior}","page":"General API","title":"Statistics.std","text":"std(\n    [rng::Random.AbstractRNG],\n    vip::AbstractVIPosterior,\n    [func = pdf],\n    t::Union{Real, AbstractVector{<:Real}},\n    [n_samples::Int=1000]\n) -> Union{Real, Vector{<:Real}}\n\nCompute the posterior standard deviation of the pdf f(t) or the cdf F(t) for each element in the collection t.\n\nIf the input t is a scalar, a scalar is returned. If t is a vector, this function returns a vector the same length as t. This method is equivalent to sqrt.(var(rng, vip, t, n_samples)).\n\n\n\n\n\n","category":"method"},{"location":"api/general_api/#BayesDensityCore.VariationalOptimizationResult","page":"General API","title":"BayesDensityCore.VariationalOptimizationResult","text":"VariationalOptimizationResult{T<:Real}\n\nStruct holding the result of a variational inference procedure.\n\nFields\n\nELBO: The values of the evidence lower bound per iteration.\nconverged: Boolean flag indicating whether the optimization was succesful or not.\nn_iter: Number of iterations run before termination.\ntolerance: Tolerance parameter used to determine convergence.\nvariational_posterior: The fitted variational posterior distribution.\n\n\n\n\n\n","category":"type"},{"location":"api/general_api/#BayesDensityCore.elbo","page":"General API","title":"BayesDensityCore.elbo","text":"elbo(varoptinf::VariationalOptimizationResult{T}) where {T} -> Vector{T}\n\nGet the value of the evidence lower bound for each iteration of the optimization procedure.\n\n\n\n\n\n","category":"function"},{"location":"api/general_api/#BayesDensityCore.n_iter","page":"General API","title":"BayesDensityCore.n_iter","text":"n_iter(varoptinf::VariationalOptimizationResult{T}) where {T} -> Int\n\nGet the number of iterations used to fit the variational posterior distribution.\n\n\n\n\n\n","category":"function"},{"location":"api/general_api/#BayesDensityCore.converged","page":"General API","title":"BayesDensityCore.converged","text":"converged(varoptinf::VariationalOptimizationResult{T}) where {T} -> Bool\n\nReturn the convergence status of the variational optimization as a bool.\n\n\n\n\n\n","category":"function"},{"location":"api/general_api/#BayesDensityCore.tolerance","page":"General API","title":"BayesDensityCore.tolerance","text":"tolerance(varoptinf::VariationalOptimizationResult{T}) where {T} -> T\n\nGet the value of the tolerance level used to determine convergence.\n\n\n\n\n\n","category":"function"},{"location":"api/general_api/#BayesDensityCore.posterior","page":"General API","title":"BayesDensityCore.posterior","text":"posterior(varoptinf::VariationalOptimizationResult{T}) where {T} -> AbstractVIPosterior{T}\n\nGet the fitted variational posterior distribution.\n\n\n\n\n\n","category":"function"},{"location":"#BayesDensity.jl","page":"Home","title":"BayesDensity.jl","text":"BayesDensity.jl is a Julia package for univariate nonparametric Bayesian density estimation. The package provides access to several density estimators from the Bayesian nonparametrics literature. For most of the implemented methods, posterior inference is possible through both Markov chain Monte Carlo (MCMC) methods and variational inference (VI).","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"We note that each of the models implemented in BayesDensity can be installed independently from all the by downloading the corresponding module. For instance, if we would like to use the HistSmoother model we need to install the BayesDensityHistSmoother package:\n\nusing Pkg\nPkg.add(url=\"https://github.com/oskarhs/BayesianDensityEstimation.jl/lib/BayesDensityHistSmoother.jl\")\n\nWe can now use the model by importing the downloaded package:\n\nusing BayesDensityHistSmoother\n\nAlternatively, if one wants to have access to more models, one can install the BayesDensity package instead:\n\nusing Pkg\nPkg.add(url=\"https://github.com/oskarhs/BayesianDensityEstimation.jl/lib/BayesDensity.jl\")\n\nWe can now import all the models implemented in this package by running the following code snippet:\n\nusing BayesDensity","category":"section"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"To illustarte the basic use of the package, we show one can fit a histogram smoother to a simulated dataset.\n\nusing BayesDensityHistSmoother, Distributions, Random\nrng = Random.Xoshiro(1) # for reproducibility\n\n# Simulate some data:\nd_true = MixtureModel([Normal(-0.2, 0.25), Normal(0.5, 0.15)], [0.4, 0.6])\nx = rand(rng, d_true, 1000)\n\n# Create a HistSmoother model object:\nsmoother = HistSmoother(x)\n\nHaving specified a model for the data, we can perform posterior inference through Markov chain Monte Carlo methods or variational inference:\n\nmcmc_fit = sample(rng, smoother, 2100; n_burnin=100) # MCMC\nvi_fit = varinf(smoother)                            # VI\n\nThe resulting fits can easily be plotted using the Plots.jl and Makie.jl package extensions. For example, the posterior mean and 95  pointwise credible bands can be plotted via Makie as follows:\n\nusing CairoMakie\nplot(mcmc_fit) # Based on MCMC\nplit(vi_fit)   # Based on VI","category":"section"},{"location":"methods/#Index","page":"Index","title":"Index","text":"The methods pages document the public API of the modules that live in the BayesDensity monorepo. To get an overview of the functionality common to all BayesDensity models, it is strongly recommended to read the general API page before studying the individual method docs.","category":"section"},{"location":"methods/#Overview","page":"Index","title":"Overview","text":"The following table provides a complete list of all the models available in BayesDensity, the corresponding module, along with the algorithms that are currently available for posterior inference.\n\nModel MCMC VI Module\nBSplineMixture ✅ ✅ BayesDensityBSplineMixture\nFiniteGaussianMixture ✅ ✅ BayesDensityFiniteGaussianMixture\nHistSmoother ✅ ✅ BayesDensityHistSmoother\nPitmanYorMixture ✅ ✅ BayesDensityPitmanYorMixture\nRandomFiniteGaussianMixture ❌ ✅ BayesDensityFiniteGaussianMixture","category":"section"},{"location":"methods/#TODO:-Add-link-to-modules-once-we-are-online.","page":"Index","title":"TODO: Add link to modules once we are online.","text":"","category":"section"}]
}
